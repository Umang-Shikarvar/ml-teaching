---
title: "Interactive Notebooks"
page-layout: full
title-block-banner: true
---

## Hands-On Learning with Jupyter Notebooks

Interactive notebooks provide practical coding examples and visualizations to complement the lecture slides. All notebooks are runnable in Google Colab or local Jupyter environments.

### Fundamentals & Data Science Basics

- [Anscombe's Quartet](notebooks/anscombe.html) - Why visualization matters
- [Visualization Techniques](notebooks/visualisation.html) - Comprehensive plotting guide
- [NumPy & Pandas Basics](notebooks/numpy-pandas-basics.html) - Essential data manipulation
- [Binomial Distribution](notebooks/binomial.html) - Probability foundations
- [Tips Dataset Analysis](notebooks/tips.html) - Real-world data exploration
- [Rule-Based vs ML](notebooks/rule-based-vs-ml.html) - When to use ML

### Mathematical Foundations

- [Basis Functions](notebooks/basis.html) - Linear algebra foundations
- [Basis Functions II](notebooks/basis2.html) - Advanced basis concepts
- [Contour Plots](notebooks/contour.html) - Visualizing functions
- [Meshgrid & Contours](notebooks/meshgrid-contour-explanation.html) - Grid visualization
- [Taylor Series](notebooks/taylor-series.html) - Mathematical approximations
- [Polynomial Features](notebooks/polynomial_features.html) - Feature engineering
- [Curse of Dimensionality](notebooks/curse-dimensionality.html) - High-dimensional problems

### Optimization & Gradient Methods

- [Gradient Descent](notebooks/Gradient Descent.html) - Core optimization
- [Gradient Descent 2D](notebooks/Gradient Descent-2d.html) - Visual optimization
- [Cost vs Iterations](notebooks/cost-iteration-notebook.html) - Convergence analysis
- [Automatic Differentiation](notebooks/autodiff.html) - Modern backpropagation

### Supervised Learning Algorithms

#### Linear Models
- [Linear Regression](notebooks/linear-regression.html) - Basic regression
- [Linear Regression Tutorial](notebooks/lin-reg-tutorial.html) - Step-by-step guide
- [Geometric Linear Regression](notebooks/geometric-linear-regression.html) - Visual interpretation
- [Ridge Regression](notebooks/Ridge.html) - Regularized regression
- [Lasso Regression](notebooks/Lasso_Regression.html) - Sparse solutions

#### Classification Methods
- [Logistic Regression](notebooks/logistic-regression-torch.html) - PyTorch implementation
- [Logistic Regression Cost](notebooks/logistic-regression-cost.html) - Loss function analysis
- [Logistic: Apples vs Oranges](notebooks/logistic-apple-oranges.html) - Binary classification
- [Logistic: Circular Data](notebooks/logistic-circular.html) - Non-linear boundaries
- [Logistic: Iris Dataset](notebooks/logistic-iris.html) - Multi-class classification
- [Logits Usage](notebooks/logits-usage.html) - Understanding logits

#### Tree-Based Methods
- [Decision Trees: Discrete → Discrete](notebooks/decision-tree-discrete-input-discrete-output.html)
- [Decision Trees: Real → Discrete](notebooks/decision-tree-real-input-discrete-output.html)
- [Decision Trees: Real → Real](notebooks/decision-tree-real-input-real-output.html)
- [Decision Tree Classes](notebooks/classes-tree.html) - Classification trees
- [Weighted Decision Trees](notebooks/dt_weighted.html) - Sample weighting
- [Ensemble Feature Importance](notebooks/ensemble-feature-importance.html)
- [Boosting Explanation](notebooks/boosting-explanation.html) - AdaBoost mechanics
- [Ensemble Representations](notebooks/representation-ensemble.html)

#### Instance-Based Learning
- [KNN Variants](notebooks/knn-variants.html) - Different distance metrics
- [Movie Recommendation](notebooks/movie-recommendation-knn-mf.html) - Collaborative filtering

#### Support Vector Machines
- [SVM Basics](notebooks/svm.html) - Introduction to SVMs
- [SVM with CVXOPT](notebooks/svm-cvxopt.html) - Optimization implementation
- [SVM Kernel Understanding](notebooks/svm-kernel-understanding.html) - Kernel trick
- [SVM Kernels](notebooks/svm-kernel.html) - Different kernel functions
- [SVM Primal-Dual](notebooks/svm-primal-dual.html) - Mathematical foundations
- [SVM Soft Margin](notebooks/svm-soft-margin.html) - Handling noise

### Model Evaluation & Selection

- [Bias-Variance Analysis](notebooks/bias-variance.html) - Fundamental tradeoff
- [Bias-Variance Charts](notebooks/bias-variance-charts.html) - Visual analysis
- [Cross-Validation Diagrams](notebooks/cross-validation-diagrams.html) - Validation strategies
- [PR Curves](notebooks/pr-curve.html) - Precision-Recall analysis
- [Dummy Baselines](notebooks/dummy-baselines.html) - Baseline comparisons
- [Dummy Variables](notebooks/dummy-variables-multi-colinearity.html) - Encoding issues
- [Hyperparameter Optimization](notebooks/hyperparameter-optimisation.html)
- [Hyperparameter Experiments](notebooks/hyperparams-experiments.html)
- [Confusion Matrix: MNIST](notebooks/confusion-mnist.html) - Multi-class evaluation

### Neural Networks & Deep Learning

- [Perceptron Learning](notebooks/perceptron-learning.html) - Basic neural unit
- [Convolutional Operations](notebooks/convolution-operation.html) - CNN fundamentals
- [Convolution with Stride](notebooks/convolution-operation-stride.html) - Parameter effects
- [CNNs](notebooks/cnn.html) - Complete CNN implementation
- [1D CNNs](notebooks/1d-cnn.html) - Sequence processing
- [CNN Edge Detection](notebooks/cnn-edge.html) - Feature visualization
- [LeNet Architecture](notebooks/lenet.html) - Classic CNN
- [VGG on MNIST](notebooks/vgg-minst.html) - Modern CNN
- [MNIST Digits](notebooks/mnist-digits.html) - Digit classification
- [Image Generation](notebooks/generate-image.html) - Generative models
- [Object Detection](notebooks/object-detection.html) - Computer vision
- [SIREN Networks](notebooks/siren.html) - Implicit neural representations
- [NN Vectorization](notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html) - Efficient implementation

### Unsupervised Learning

- [Principal Component Analysis](notebooks/pca.html) - Dimensionality reduction
- [Tensor Factorization](notebooks/tensor-factorisation.html) - Matrix decomposition

### Advanced Topics

- [Autoregressive Models](notebooks/autoregressive_model.html) - Time series prediction
- [Q-Learning](notebooks/rl-qlearning.html) - Reinforcement learning basics
- [Deep Q-Learning](notebooks/rl-qlearning-deep.html) - Neural RL
- [RL Gym Environments](notebooks/rl-gym-environments.html) - Practice environments
- [Feature Selection](notebooks/SFS_and_BFS.html) - Forward/backward selection
- [Sklearn on GPU](notebooks/Sklearn_on_GPU.html) - Performance optimization
- [Zero/Few Shot Learning](notebooks/ZeroShot_FewShot.html) - Modern ML paradigms

### Special Applications

- [Names Analysis](notebooks/names.html) - Text processing
- [Audio Transcription](notebooks/transcript.html) - Speech processing

---

## Getting Started

### Running Notebooks
1. **Google Colab**: Click any notebook link above
2. **Local Jupyter**: Clone the repository and run `jupyter notebook`
3. **Binder**: Launch interactive environment (link coming soon)

### Prerequisites
- Basic Python knowledge
- Familiarity with NumPy, Pandas, Matplotlib
- Linear algebra and calculus basics

## Tips for Success
- **Start Sequential**: Begin with Fundamentals before diving into advanced topics
- **Practice Regularly**: Code along with each notebook
- **Experiment**: Modify parameters and observe changes
- **Visualize**: Pay attention to plots and visual explanations

## Quick Navigation
- [Lecture Slides](slides.qmd) - Theoretical foundations
- [Course Homepage](index.qmd) - Main course page
- [Instructor](https://nipunbatra.github.io) - Nipun Batra's homepage