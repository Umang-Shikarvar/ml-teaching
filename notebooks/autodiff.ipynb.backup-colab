{
 "cells": [
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: Nipun Batra\n",
    "date: 2023-04-04\n",
    "description: Autodiff Helper\n",
    "tags: [ML]\n",
    "title: 'Autodiff'\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Torch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f'(2) = 14.1143798828125\n",
      "f'(2) = 14.1143798828125\n"
     ]
    }
   ],
   "source": [
    "### Derviatives using numerical differentiation\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 + 2 * x + 1\n",
    "\n",
    "def numerical_derivative_single_side(f, x, h=0.001):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def numerical_derivative_double_side(f, x, h=0.001):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=False)\n",
    "print(f'f\\'(2) = {numerical_derivative_single_side(f, x, 0.00001)}')\n",
    "print(f'f\\'(2) = {numerical_derivative_double_side(f, x, 0.00001)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9918)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(theta_0, theta_1, theta_2, theta_3, theta_4):\n",
    "    return theta_0 + 2 * theta_1 + 3 * theta_2 + 4 * theta_3 + 5 * theta_4\n",
    "\n",
    "\n",
    "\n",
    "theta_0 = torch.tensor(1.0, requires_grad=False)\n",
    "theta_1 = torch.tensor(2.0, requires_grad=False)\n",
    "theta_2 = torch.tensor(3.0, requires_grad=False)\n",
    "theta_3 = torch.tensor(4.0, requires_grad=False)\n",
    "theta_4 = torch.tensor(5.0, requires_grad=False)\n",
    "\n",
    "\n",
    "df_dtheta_0 = numerical_derivative_single_side(lambda theta_0: f(theta_0, theta_1, theta_2, theta_3, theta_4), theta_0, 0.0001)\n",
    "df_dtheta_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Above method is very expensive and not practical for large number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0 = torch.tensor(1.0, requires_grad=True)\n",
    "theta_1 = torch.tensor(1.0, requires_grad=True)\n",
    "theta_2 = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "x1 = torch.tensor(1.0)\n",
    "x2 = torch.tensor(2.0)\n",
    "\n",
    "f1 = theta_1*x1\n",
    "f2 = theta_2*x2\n",
    "\n",
    "f3 = f1 + f2\n",
    "\n",
    "f4 = f3 + theta_0\n",
    "\n",
    "f5 = f4*-1\n",
    "\n",
    "f6 = torch.exp(f5)\n",
    "\n",
    "f7 = 1 + f6\n",
    "\n",
    "f8 = 1/f7\n",
    "\n",
    "f9 = torch.log(f8)\n",
    "\n",
    "L = f9*-1\n",
    "\n",
    "all_nodes = {\"theta_0\": theta_0, \"theta_1\": theta_1, \"theta_2\": theta_2,  \n",
    "             \"f1\": f1, \"f2\": f2, \"f3\": f3, \"f4\": f4, \"f5\": f5, \"f6\": f6, \"f7\": f7, \"f8\": f8, \"f9\": f9, \"L\": L}\n",
    "\n",
    "# Retain grad for all nodes\n",
    "for node in all_nodes.values():\n",
    "    node.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0: 1.0\n",
      "theta_1: 1.0\n",
      "theta_2: 2.0\n",
      "f1: 1.0\n",
      "f2: 4.0\n",
      "f3: 5.0\n",
      "f4: 6.0\n",
      "f5: -6.0\n",
      "f6: 0.0024787522852420807\n",
      "f7: 1.0024787187576294\n",
      "f8: 0.9975274205207825\n",
      "f9: -0.0024756414350122213\n",
      "L: 0.0024756414350122213\n"
     ]
    }
   ],
   "source": [
    "# Print out the function evaluation for all nodes along with name of the node\n",
    "for name, node in all_nodes.items():\n",
    "    print(f\"{name}: {node.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0: -0.00247262348420918\n",
      "theta_1: -0.00247262348420918\n",
      "theta_2: -0.00494524696841836\n",
      "f1: -0.00247262348420918\n",
      "f2: -0.00247262348420918\n",
      "f3: -0.00247262348420918\n",
      "f4: -0.00247262348420918\n",
      "f5: 0.00247262348420918\n",
      "f6: 0.9975274801254272\n",
      "f7: 0.9975274801254272\n",
      "f8: -1.0024787187576294\n",
      "f9: -1.0\n",
      "L: 1.0\n"
     ]
    }
   ],
   "source": [
    "L.backward()\n",
    "\n",
    "# Print out the gradient for all nodes along with name of the node\n",
    "for name, node in all_nodes.items():\n",
    "    print(f\"{name}: {node.grad.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9975, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1/(f7**2))*-1.00247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0025, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(f5)*0.9975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Micrograd demo: https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example to illustrate accumulation of gradients\n",
    "\n",
    "theta = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "x1 = torch.tensor(1.0)\n",
    "x2 = torch.tensor(2.0)\n",
    "\n",
    "L1 = theta*x1\n",
    "L2 = theta*x2\n",
    "\n",
    "L = L1 + L2\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need to use torch.no_grad() in the test phase?\n",
    "### Why do we need to zero out the gradients in the training phase after each update?\n",
    "\n",
    "```py\n",
    "model = SimpleModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data\n",
    "inputs = torch.randn((100, 10))\n",
    "targets = torch.randn((100, 1))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8cc4387c5b88b39f1b3debd9ce6bc55d7689e868752fadede6c104bfdbfe39e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
