\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage[bookmarks=false]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric, trees}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}

\pagestyle{fancy}
\fancyhf{}
\rhead{Tutorial: Decision Trees}
\lhead{ES335 - Machine Learning}
\rfoot{Page \thepage}

\titleformat{\section}{\Large\bfseries\color{blue!75!black}}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\color{blue!60!black}}{\thesubsection}{1em}{}

\usepackage{../../shared/styles/conventions}

\newcounter{exercise}
\setcounter{exercise}{0}

\title{\textbf{Tutorial: Decision Trees} \\ \textit{Cheat Sheet and Practice Problems}}
\author{ES335 - Machine Learning \\ IIT Gandhinagar}
\date{\today}

\begin{document}

\maketitle

\section{Summary from Slides}

\subsection{Key Concepts}

\textbf{What are Decision Trees?}
\begin{itemize}
    \item Tree-like model for making decisions through series of questions
    \item Internal nodes: feature tests, Leaves: class labels (classification) or values (regression)
    \item Interpretable - can trace prediction path
\end{itemize}

\textbf{Types of Decision Trees:}
\begin{itemize}
    \item \textbf{Discrete Input, Discrete Output}: Classification with categorical features
    \item \textbf{Discrete Input, Real Output}: Regression with categorical features  
    \item \textbf{Real Input, Discrete Output}: Classification with continuous features
    \item \textbf{Real Input, Real Output}: Regression with continuous features
\end{itemize}

\subsection{Information Theory}

\textbf{Entropy}: Measure of impurity/uncertainty
$$H(S) = -\sum_{i=1}^c p_i \log_2(p_i)$$

\textbf{Information Gain}: Reduction in entropy after split
$$\text{Gain}(S, A) = H(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} H(S_v)$$

\textbf{Weighted Entropy}: For weighted samples
$$H_w(S) = -\sum_{i=1}^c p_{w,i} \log_2(p_{w,i})$$
where $p_{w,i} = \frac{\sum_{j: y_j = i} w_j}{\sum_{j} w_j}$

\subsection{Tree Construction Algorithm}

\begin{enumerate}
    \item Calculate entropy of current node
    \item For each feature, calculate information gain
    \item Choose feature with maximum information gain
    \item Split on that feature
    \item Recursively apply to child nodes
    \item Stop when: pure node, max depth reached, min samples reached
\end{enumerate}

\textbf{For Regression Trees}: Use variance reduction instead of information gain
$$\text{VarReduction}(S, A) = \text{Var}(S) - \sum_{v} \frac{|S_v|}{|S|} \text{Var}(S_v)$$

\subsection{Key Properties}

\textbf{Advantages}:
\begin{itemize}
    \item Highly interpretable
    \item No assumptions about data distribution
    \item Handles both numerical and categorical features
    \item Automatic feature selection
    \item Non-parametric
\end{itemize}

\textbf{Disadvantages}:
\begin{itemize}
    \item Prone to overfitting
    \item Unstable (small data changes can drastically change tree)
    \item Biased toward features with more levels
    \item Greedy algorithm (may not find globally optimal tree)
\end{itemize}

\section{Practice Problems}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Information Gain Calculation]

Given the tennis dataset from slides:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Outlook} & \textbf{Temperature} & \textbf{Humidity} & \textbf{Wind} & \textbf{Play?} \\
\hline
Sunny & Hot & High & Weak & No \\
Sunny & Hot & High & Strong & No \\
Overcast & Hot & High & Weak & Yes \\
Rain & Mild & High & Weak & Yes \\
Rain & Cool & Normal & Weak & Yes \\
Rain & Cool & Normal & Strong & No \\
Overcast & Cool & Normal & Strong & Yes \\
Sunny & Mild & High & Weak & No \\
Sunny & Cool & Normal & Weak & Yes \\
Rain & Mild & Normal & Weak & Yes \\
Sunny & Mild & Normal & Strong & Yes \\
Overcast & Mild & High & Strong & Yes \\
Overcast & Hot & Normal & Weak & Yes \\
Rain & Mild & High & Strong & No \\
\hline
\end{tabular}
\end{center}

Calculate the information gain for splitting on the "Wind" attribute.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Tree Construction]

Build a complete decision tree for this simple dataset:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{X1} & \textbf{X2} & \textbf{Y} \\
\hline
0 & 0 & A \\
0 & 1 & B \\
1 & 0 & B \\
1 & 1 & A \\
\hline
\end{tabular}
\end{center}

Show all entropy calculations and explain your splitting decisions.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Regression Tree]

For this regression dataset:
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{Size} & \textbf{Price} \\
\hline
1000 & 100 \\
1200 & 120 \\
1500 & 180 \\
1800 & 200 \\
2000 & 250 \\
\hline
\end{tabular}
\end{center}

Find the best split point using variance reduction. Calculate the variance before and after the split.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Weighted Entropy]

Given a weighted dataset where samples have weights [2, 1, 3, 1] and classes [A, B, A, B]:

a) Calculate the weighted entropy
b) Compare with unweighted entropy
c) Explain when weighted entropy is useful
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Overfitting Analysis]

You have a decision tree with the following training and validation accuracies at different depths:

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Depth} & \textbf{Training Acc} & \textbf{Validation Acc} \\
\hline
1 & 0.70 & 0.68 \\
2 & 0.80 & 0.76 \\
3 & 0.90 & 0.82 \\
4 & 0.95 & 0.79 \\
5 & 0.98 & 0.75 \\
6 & 1.00 & 0.70 \\
\hline
\end{tabular}
\end{center}

a) At what depth does overfitting begin?
b) What depth would you choose for deployment?
c) Suggest three techniques to reduce overfitting.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Feature Selection Bias]

Explain why decision trees are biased toward features with more distinct values. Give a concrete example with:
- Feature A: 2 possible values
- Feature B: 10 possible values
- Same information content

How would you address this bias?
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Continuous Feature Splits]

For a continuous feature "Age" with values [25, 30, 35, 40, 45, 50] and binary target [0, 0, 1, 1, 0, 1]:

a) List all possible split points
b) Calculate information gain for split at Age $\leq$ 37.5
c) Find the optimal split point
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Pruning Strategies]

Compare pre-pruning vs post-pruning:

a) Define minimum samples per leaf = 3. How does this affect tree growth?
b) For post-pruning, explain how you would use a validation set
c) What is the trade-off between model complexity and generalization?
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Missing Values]

How would you handle missing values in decision trees during:
a) Training phase
b) Prediction phase

Propose two different strategies for each phase and discuss their pros/cons.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Implementation Challenge]

Implement the decision tree algorithm pseudocode:

\begin{verbatim}
function BuildTree(data, features, target):
    if stopping_condition(data):
        return create_leaf(data)
    
    best_feature = find_best_split(data, features)
    tree = create_node(best_feature)
    
    for each value v in best_feature:
        subset = filter_data(data, best_feature == v)
        subtree = BuildTree(subset, features, target)
        add_branch(tree, v, subtree)
    
    return tree
\end{verbatim}

Define each helper function and specify the stopping conditions.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Ensemble Motivation]

Given the instability of decision trees:

a) Explain why small changes in data can lead to very different trees
b) How does this motivate ensemble methods like Random Forests?
c) Describe the bias-variance trade-off for decision trees
d) How do ensembles help with this trade-off?
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Real-World Application]

Design a decision tree for a loan approval system with features:
- Income (continuous)
- Credit Score (continuous) 
- Employment Type (categorical: Full-time, Part-time, Self-employed, Unemployed)
- Previous Defaults (binary)

a) What would be appropriate stopping criteria?
b) How would you handle class imbalance (more approvals than rejections)?
c) What interpretability constraints might be required for regulatory compliance?
d) How would you validate the model's fairness across different demographic groups?
\end{tcolorbox}

\end{document}