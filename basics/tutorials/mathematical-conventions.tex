\documentclass{article}
\usepackage[utf8]{inputenc}

% Essential packages for article format (must load before conventions)
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{bm}

% Load conventions after math packages
\usepackage{../../shared/styles/conventions}

% Manual definition of key commands in case package loading fails
\providecommand{\vx}{\mathbf{x}}
\providecommand{\vy}{\mathbf{y}} 
\providecommand{\vw}{\mathbf{w}}
\providecommand{\vtheta}{\boldsymbol{\theta}}
\providecommand{\mX}{\mathbf{X}}
\providecommand{\mY}{\mathbf{Y}}
\providecommand{\mZ}{\mathbf{Z}}
\providecommand{\mA}{\mathbf{A}}
\providecommand{\mB}{\mathbf{B}}
\providecommand{\mI}{\mathbf{I}}
\providecommand{\mSigma}{\boldsymbol{\Sigma}}
\providecommand{\Real}{\mathbb{R}}
\providecommand{\MSE}{\operatorname{MSE}}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{enumitem}

% Additional packages for a nice tutorial format
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{enumitem}

% Header and footer setup
\pagestyle{fancy}
\fancyhf{}
\rhead{ML Mathematical Conventions Tutorial}
\lhead{ES335 - Machine Learning}
\rfoot{Page \thepage}

% Title formatting
\titleformat{\section}{\Large\bfseries\color{blue!75!black}}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\color{blue!60!black}}{\thesubsection}{1em}{}

% Define counter for exercises
\newcounter{exercise}
\setcounter{exercise}{0}

% Define counter for coding problems
\newcounter{coding}
\setcounter{coding}{0}

\title{\textbf{Mathematical Conventions and Fundamentals} \\ \textit{Complete Guide to Mathematical Notation and Concepts}}
\author{ES335 - Machine Learning \\ IIT Gandhinagar}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This comprehensive tutorial covers essential mathematical notation, conventions, and fundamental concepts needed for machine learning. It includes basic mathematical notation (scalars, vectors, matrices), advanced mathematical operations (derivatives, norms, matrix operations), evaluation metrics for ML, and practical exercises. Understanding these concepts is crucial for following lectures, reading research papers, and implementing algorithms.
\end{abstract}

\tableofcontents
\newpage

\section{Scalar Notation}

\subsection{Basic Scalars}
A \textbf{scalar} is a single number. We typically use lowercase letters to denote scalars:
\begin{itemize}
    \item $a, b, c$ - generic scalars
    \item $\alpha, \beta, \gamma$ - Greek letters for parameters
    \item $n, m, d$ - dimensions and counts
    \item $\epsilon$ - small positive number (tolerance)
    \item $\lambda$ - regularization parameter
\end{itemize}

\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Example]
If we have a learning rate $\alpha = 0.01$ and a regularization parameter $\lambda = 0.1$, both are scalars.
\end{tcolorbox}

\subsection{Special Scalars}
\begin{itemize}
    \item $y_i$ - the $i$-th target value (scalar output)
    \item $\hat{y}_i$ - predicted value for the $i$-th sample
    \item $\ell(\hat{y}_i, y_i)$ - loss function (returns a scalar)
\end{itemize}

\section{Vector Notation}

\subsection{Vector Representation}
A \textbf{vector} is an ordered list of numbers. We use lowercase bold letters or arrows:
\begin{itemize}
    \item $\vx, \vy, \vz$ - generic vectors
    \item $\vw$ - weight vector (parameters)
    \item $\vtheta$ - parameter vector
    \item $\valpha$ - vector of dual variables
\end{itemize}

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Vector Examples]
\begin{align}
\vx &= \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix} \in \Real^d \\
\vw &= \begin{bmatrix} w_1 \\ w_2 \\ w_3 \end{bmatrix} \in \Real^3
\end{align}
\end{tcolorbox}

\subsection{Vector Operations}
\begin{itemize}
    \item $\vx \cdot \vy = \sum_{i=1}^d x_i y_i$ - dot product (inner product)
    \item $\|\vx\|$ - norm of vector $\vx$
    \item $\|\vx\|_2 = \sqrt{\sum_{i=1}^d x_i^2}$ - L2 norm (Euclidean norm)
    \item $\|\vx\|_1 = \sum_{i=1}^d |x_i|$ - L1 norm (Manhattan norm)
    \item $\vx^T$ - transpose of vector (row vector)
\end{itemize}

\section{Matrix Notation}

\subsection{Matrix Representation}
A \textbf{matrix} is a rectangular array of numbers. We use uppercase bold letters:
\begin{itemize}
    \item $\mX, \mY, \mZ$ - generic matrices  
    \item $\mA, \mB$ - coefficient matrices
    \item $\mI$ - identity matrix
    \item $\mSigma$ - covariance matrix
\end{itemize}

\begin{tcolorbox}[colback=orange!5!white,colframe=orange!75!black,title=Matrix Example]
\begin{align}
\mX = \begin{bmatrix} 
x_{11} & x_{12} & \cdots & x_{1d} \\
x_{21} & x_{22} & \cdots & x_{2d} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{nd}
\end{bmatrix} \in \Real^{n \times d}
\end{align}
where $n$ is the number of samples and $d$ is the number of features.
\end{tcolorbox}

\subsection{Matrix Operations}
\begin{itemize}
    \item $\mA^T$ - transpose of matrix $\mA$
    \item $\mA^{-1}$ - inverse of matrix $\mA$ (if it exists)
    \item $\mA\mB$ - matrix multiplication
    \item $\text{tr}(\mA)$ - trace of matrix $\mA$
    \item $\det(\mA)$ - determinant of matrix $\mA$
\end{itemize}

\section{Common Mathematical Spaces}

\subsection{Real Number Spaces}
\begin{itemize}
    \item $\Real$ - the set of all real numbers
    \item $\Real^d$ - $d$-dimensional real vector space
    \item $\Real^{n \times d}$ - space of $n \times d$ real matrices
    \item $\Real_+$ - positive real numbers
    \item $\Real_{++}$ - strictly positive real numbers
\end{itemize}

\begin{tcolorbox}[colback=purple!5!white,colframe=purple!75!black,title=Space Examples]
\begin{itemize}
    \item A house price: $p \in \Real_+$ (positive real number)
    \item Feature vector: $\vx \in \Real^d$ (d-dimensional vector)
    \item Dataset: $\mX \in \Real^{n \times d}$ (n samples, d features)
\end{itemize}
\end{tcolorbox}

\subsection{Other Important Spaces}
\begin{itemize}
    \item $\{0, 1\}$ - binary values
    \item $\{1, 2, \ldots, k\}$ - discrete classes (for k-class classification)
    \item $[0, 1]$ - unit interval (for probabilities)
\end{itemize}

\section{Dataset and ML-Specific Notation}

\subsection{Dataset Representation}
\begin{itemize}
    \item $\cD = \{(\vx_i, y_i)\}_{i=1}^n$ - dataset with $n$ samples
    \item $\vx_i \in \Real^d$ - $i$-th feature vector (input)
    \item $y_i \in \Real$ - $i$-th target value (for regression)
    \item $y_i \in \{1, 2, \ldots, k\}$ - $i$-th class label (for classification)
    \item $\hat{y}_i$ - predicted output for sample $i$
\end{itemize}

\subsection{Model Parameters}
\begin{itemize}
    \item $\vtheta \in \Real^p$ - parameter vector with $p$ parameters
    \item $\vw \in \Real^d$ - weight vector
    \item $b \in \Real$ - bias term (intercept)
    \item $f(\vx; \vtheta)$ - model function parameterized by $\vtheta$
\end{itemize}

\section{Exercises}

\subsection{Theoretical Exercises}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Exercise \stepcounter{exercise}\#\theexercise: Vector Operations]
Given vectors $\vx = [2, -1, 3]^T$ and $\vy = [1, 4, -2]^T$:
\begin{enumerate}[label=(\alph*)]
    \item Calculate the dot product $\vx \cdot \vy$
    \item Find the L2 norm $\|\vx\|_2$
    \item Compute the L1 norm $\|\vy\|_1$
\end{enumerate}

\textbf{Solutions:}
\begin{enumerate}[label=(\alph*)]
    \item $\vx \cdot \vy = 2(1) + (-1)(4) + 3(-2) = 2 - 4 - 6 = -8$
    \item $\|\vx\|_2 = \sqrt{2^2 + (-1)^2 + 3^2} = \sqrt{4 + 1 + 9} = \sqrt{14}$
    \item $\|\vy\|_1 = |1| + |4| + |-2| = 1 + 4 + 2 = 7$
\end{enumerate}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Exercise \stepcounter{exercise}\#\theexercise: Matrix Dimensions]
For the following expressions, determine if they are valid and find the dimensions:
\begin{enumerate}[label=(\alph*)]
    \item $\mA \in \Real^{3 \times 4}$, $\mB \in \Real^{4 \times 2}$. What is the dimension of $\mA\mB$?
    \item $\vx \in \Real^5$, $\mW \in \Real^{3 \times 5}$. What is the dimension of $\mW\vx$?
    \item $\vx \in \Real^d$, $\vy \in \Real^d$. What is the dimension of $\vx^T\vy$?
\end{enumerate}

\textbf{Solutions:}
\begin{enumerate}[label=(\alph*)]
    \item Valid. $\mA\mB \in \Real^{3 \times 2}$
    \item Valid. $\mW\vx \in \Real^3$
    \item Valid. $\vx^T\vy \in \Real$ (scalar)
\end{enumerate}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Exercise \stepcounter{exercise}\#\theexercise: ML Notation]
Given a dataset with 100 samples and 5 features:
\begin{enumerate}[label=(\alph*)]
    \item Write the dimensions of the feature matrix $\mX$
    \item If this is a regression problem, what are the dimensions of the target vector $\vy$?
    \item If the weight vector is $\vw \in \Real^5$ and bias is $b \in \Real$, write the prediction for sample $i$
\end{enumerate}

\textbf{Solutions:}
\begin{enumerate}[label=(\alph*)]
    \item $\mX \in \Real^{100 \times 5}$
    \item $\vy \in \Real^{100}$
    \item $\hat{y}_i = \vw^T \vx_i + b$ or $\hat{y}_i = \vx_i^T \vw + b$
\end{enumerate}
\end{tcolorbox}

\subsection{Coding Exercises}

\begin{tcolorbox}[colback=cyan!5!white,colframe=cyan!75!black,title=Coding Problem \stepcounter{coding}\#\thecoding: Vector Operations in Python]
Implement the following functions in Python using NumPy:

\begin{verbatim}
import numpy as np

def compute_dot_product(x, y):
    """Compute dot product of two vectors"""
    # TODO: Implement
    pass

def compute_l2_norm(x):
    """Compute L2 norm of a vector"""
    # TODO: Implement  
    pass

def compute_l1_norm(x):
    """Compute L1 norm of a vector"""
    # TODO: Implement
    pass

# Test your functions
x = np.array([2, -1, 3])
y = np.array([1, 4, -2])

print(f"Dot product: {compute_dot_product(x, y)}")
print(f"L2 norm of x: {compute_l2_norm(x)}")
print(f"L1 norm of y: {compute_l1_norm(y)}")
\end{verbatim}

\textbf{Solution:}
\begin{verbatim}
def compute_dot_product(x, y):
    return np.dot(x, y)  # or x @ y

def compute_l2_norm(x):
    return np.linalg.norm(x, ord=2)  # or np.sqrt(np.sum(x**2))

def compute_l1_norm(x):
    return np.linalg.norm(x, ord=1)  # or np.sum(np.abs(x))
\end{verbatim}
\end{tcolorbox}

\begin{tcolorbox}[colback=cyan!5!white,colframe=cyan!75!black,title=Coding Problem \stepcounter{coding}\#\thecoding: Dataset Creation]
Create a synthetic dataset and verify its dimensions:

\begin{verbatim}
import numpy as np

def create_regression_dataset(n_samples, n_features, noise_std=0.1):
    """Create a synthetic regression dataset"""
    # TODO: Create feature matrix X
    # TODO: Create true weight vector w
    # TODO: Create target vector y = X @ w + noise
    # Return X, y, w
    pass

# Test with 50 samples, 3 features
X, y, w_true = create_regression_dataset(50, 3)

print(f"X shape: {X.shape}")
print(f"y shape: {y.shape}")  
print(f"w_true shape: {w_true.shape}")

# Verify dimensions are correct for linear model
print(f"X @ w_true shape: {(X @ w_true).shape}")
\end{verbatim}

\textbf{Solution:}
\begin{verbatim}
def create_regression_dataset(n_samples, n_features, noise_std=0.1):
    np.random.seed(42)  # For reproducibility
    X = np.random.randn(n_samples, n_features)
    w = np.random.randn(n_features)
    noise = np.random.normal(0, noise_std, n_samples)
    y = X @ w + noise
    return X, y, w
\end{verbatim}
\end{tcolorbox}

\begin{tcolorbox}[colback=cyan!5!white,colframe=cyan!75!black,title=Coding Problem \stepcounter{coding}\#\thecoding: Linear Model Implementation]
Implement a simple linear regression model:

\begin{verbatim}
class LinearRegression:
    def __init__(self):
        self.w = None
        self.b = None
    
    def fit(self, X, y):
        """Fit linear regression using normal equation"""
        # Add bias column to X
        # TODO: Implement normal equation solution
        pass
    
    def predict(self, X):
        """Make predictions"""
        # TODO: Implement prediction
        pass
    
    def mse(self, X, y):
        """Compute mean squared error"""
        # TODO: Implement MSE calculation
        pass

# Test your implementation
X, y, _ = create_regression_dataset(100, 2)
model = LinearRegression()
model.fit(X, y)
predictions = model.predict(X)
error = model.mse(X, y)
print(f"MSE: {error}")
\end{verbatim}
\end{tcolorbox}

\section{Common Mistakes and Tips}

\subsection{Notation Mistakes to Avoid}
\begin{enumerate}
    \item \textbf{Scalar vs Vector confusion:} Always check if you're dealing with scalars (lowercase) or vectors (bold lowercase)
    \item \textbf{Matrix dimension mismatch:} Always verify matrix multiplication dimensions: $(m \times n) \times (n \times p) = (m \times p)$
    \item \textbf{Transpose confusion:} Remember $\vx^T \in \Real^{1 \times d}$ is a row vector, $\vx \in \Real^d$ is a column vector
    \item \textbf{Index notation:} $x_i$ is the $i$-th element, $\vx_i$ is the $i$-th vector
\end{enumerate}

\subsection{Best Practices}
\begin{enumerate}
    \item Always write down matrix/vector dimensions when working through problems
    \item Use consistent notation throughout your work
    \item When coding, use meaningful variable names that reflect the mathematical notation
    \item Verify your implementations with simple test cases where you know the answer
\end{enumerate}

\section{Reference Quick Sheet}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Notation} & \textbf{Type} & \textbf{Example/Description} \\
\hline
$a, \alpha, \lambda$ & Scalar & Single number \\
$\vx, \vw, \vtheta$ & Vector & Column vector in $\Real^d$ \\
$\mX, \mA, \mSigma$ & Matrix & 2D array in $\Real^{n \times d}$ \\
$\Real$ & Space & Set of real numbers \\
$\Real^d$ & Space & d-dimensional real vectors \\
$\Real^{n \times d}$ & Space & n×d real matrices \\
$\vx \cdot \vy$ & Operation & Dot product (inner product) \\
$\|\vx\|$ & Operation & Vector norm \\
$\mA^T$ & Operation & Matrix transpose \\
$\cD$ & Set & Dataset \\
$y_i, \hat{y}_i$ & Scalar & True/predicted output \\
\hline
\end{tabular}
\caption{Mathematical Notation Reference}
\end{table}

\section{Advanced Mathematical Operations}

\subsection{Vector and Matrix Norms}
\textbf{Vector Norms:}
\begin{itemize}
	\item \textbf{L1 Norm (Manhattan):} $\|\vx\|_1 = \sum_{i=1}^d |x_i|$
	\item \textbf{L2 Norm (Euclidean):} $\|\vx\|_2 = \sqrt{\sum_{i=1}^d x_i^2}$
	\item \textbf{L$\infty$ Norm (Maximum):} $\|\vx\|_\infty = \max_i |x_i|$
	\item \textbf{General Lp Norm:} $\|\vx\|_p = \left(\sum_{i=1}^d |x_i|^p\right)^{1/p}$
\end{itemize}

\textbf{Matrix Norms:}
\begin{itemize}
	\item \textbf{Frobenius Norm:} $\|\mA\|_F = \sqrt{\sum_{i,j} a_{ij}^2}$
	\item \textbf{Spectral Norm:} $\|\mA\|_2$ = largest singular value
\end{itemize}

\subsection{Matrix Operations and Properties}
\textbf{Basic Operations:}
\begin{itemize}
	\item \textbf{Transpose:} $(\mA\mB)^T = \mB^T\mA^T$
	\item \textbf{Sum of squares:} $\sum_{i=1}^n \epsilon_i^2 = \boldsymbol{\epsilon}^T\boldsymbol{\epsilon}$
	\item \textbf{Scalar property:} For scalar $s$: $s = s^T$
\end{itemize}

\textbf{Matrix Rank and Invertibility:}
\begin{itemize}
	\item \textbf{Rank:} Maximum number of linearly independent rows/columns
	\item \textbf{Full Rank:} $\text{rank}(\mA) = \min(m,n)$ for $\mA \in \Real^{m \times n}$
	\item \textbf{Invertible:} Square matrix $\mA$ is invertible if $\text{rank}(\mA) = n$
	\item \textbf{Singular:} Matrix with no inverse (determinant = 0)
\end{itemize}

\subsection{Calculus for Machine Learning}
\textbf{Derivative of Scalar w.r.t. Vector:}
If $s$ is a scalar and $\vtheta \in \Real^n$:
$$\frac{\partial s}{\partial \vtheta} = \begin{bmatrix} \frac{\partial s}{\partial \theta_1} \\ \frac{\partial s}{\partial \theta_2} \\ \vdots \\ \frac{\partial s}{\partial \theta_n} \end{bmatrix}$$

\textbf{Important Derivative Rules:}
\begin{itemize}
	\item \textbf{Linear form:} $\frac{\partial}{\partial \vtheta}(\mA\vtheta) = \mA^T$
	\item \textbf{Quadratic form:} $\frac{\partial}{\partial \vtheta}(\vtheta^T\mZ\vtheta) = 2\mZ\vtheta$ (when $\mZ^T = \mZ$)
	\item \textbf{Norm squared:} $\frac{\partial}{\partial \vtheta}\|\vtheta\|^2 = 2\vtheta$
\end{itemize}

\section{Machine Learning Metrics and Evaluation}

\subsection{Classification Metrics}
For classification problems with predictions $\hat{\vy}$ and true labels $\vy$:

\textbf{Basic Metrics:}
\begin{itemize}
	\item \textbf{Accuracy:} $\frac{|\{i : y_i = \hat{y}_i\}|}{n} = \frac{\sum_{i=1}^n \mathbf{1}[y_i = \hat{y}_i]}{n}$
	\item \textbf{Error Rate:} $1 - \text{Accuracy}$
\end{itemize}

\textbf{Confusion Matrix Metrics:}
For binary classification (Positive/Negative classes):
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Predicted +} & \textbf{Predicted -} \\
\hline
\textbf{Actual +} & TP & FN \\
\hline
\textbf{Actual -} & FP & TN \\
\hline
\end{tabular}
\end{center}

\begin{itemize}
	\item \textbf{Precision:} $P = \frac{TP}{TP + FP}$ ("Of predicted positives, how many are correct?")
	\item \textbf{Recall (Sensitivity):} $R = \frac{TP}{TP + FN}$ ("Of actual positives, how many found?")
	\item \textbf{Specificity:} $\frac{TN}{TN + FP}$ ("Of actual negatives, how many found?")
	\item \textbf{F1-Score:} $F_1 = \frac{2PR}{P + R} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}$
\end{itemize}

\subsection{Regression Metrics}
For regression with predictions $\hat{\vy}$ and true values $\vy$:

\begin{itemize}
	\item \textbf{Mean Squared Error:} $\MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2$
	\item \textbf{Root Mean Squared Error:} $\text{RMSE} = \sqrt{\MSE}$
	\item \textbf{Mean Absolute Error:} $\text{MAE} = \frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|$
	\item \textbf{Mean Error:} $\text{ME} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)$ (can cancel out!)
	\item \textbf{R-squared:} $R^2 = 1 - \frac{\sum_i (y_i - \hat{y}_i)^2}{\sum_i (y_i - \bar{y})^2}$
\end{itemize}

\section{Advanced Exercises}

\subsection{Mathematical Operations Practice}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Exercise \stepcounter{exercise}\#\theexercise: Matrix Derivatives]
Given $\vtheta = [\theta_1, \theta_2]^T$ and $\mA = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$:
\begin{enumerate}[label=(\alph*)]
	\item Calculate $\frac{\partial}{\partial \vtheta}(\mA\vtheta)$
	\item Calculate $\frac{\partial}{\partial \vtheta}(\vtheta^T\mA\vtheta)$
	\item Verify $\mA^T = \mA$ and explain why this matters
\end{enumerate}

\textbf{Solutions:}
\begin{enumerate}[label=(\alph*)]
	\item $\frac{\partial}{\partial \vtheta}(\mA\vtheta) = \mA^T = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}$
	\item $\frac{\partial}{\partial \vtheta}(\vtheta^T\mA\vtheta) = 2\mA\vtheta = 2\begin{bmatrix} 2\theta_1 + \theta_2 \\ \theta_1 + 3\theta_2 \end{bmatrix}$
	\item Yes, $\mA$ is symmetric, so the quadratic form derivative rule applies directly
\end{enumerate}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Exercise \stepcounter{exercise}\#\theexercise: Norm Calculations]
For vector $\vx = [3, -4, 0, 5]^T$:
\begin{enumerate}[label=(\alph*)]
	\item Calculate L1, L2, and L$\infty$ norms
	\item Which norm is most sensitive to outliers and why?
	\item Express $\|\vx\|_2^2$ in terms of vector operations
\end{enumerate}

\textbf{Solutions:}
\begin{enumerate}[label=(\alph*)]
	\item $\|\vx\|_1 = 12$, $\|\vx\|_2 = \sqrt{50} = 5\sqrt{2}$, $\|\vx\|_\infty = 5$
	\item L2 norm (squares amplify large values), then L$\infty$, then L1
	\item $\|\vx\|_2^2 = \vx^T\vx = 50$
\end{enumerate}
\end{tcolorbox}

\subsection{ML Metrics Practice}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Exercise \stepcounter{exercise}\#\theexercise: Classification Metrics]
Given confusion matrix for a binary classifier:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
 & \textbf{Pred +} & \textbf{Pred -} \\
\hline
\textbf{Actual +} & 85 & 15 \\
\hline
\textbf{Actual -} & 10 & 90 \\
\hline
\end{tabular}
\end{center}
Calculate: (a) Accuracy (b) Precision (c) Recall (d) F1-score

\textbf{Solutions:}
\begin{enumerate}[label=(\alph*)]
	\item Accuracy = $(85+90)/200 = 87.5\%$
	\item Precision = $85/(85+10) = 89.5\%$
	\item Recall = $85/(85+15) = 85\%$
	\item F1-score = $2 \times 0.895 \times 0.85 / (0.895 + 0.85) = 87.2\%$
\end{enumerate}
\end{tcolorbox}

\section{Reference Quick Sheet}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Category} & \textbf{Notation} & \textbf{Meaning} \\
\hline
\multirow{4}{*}{Scalars} & $a, b, c$ & Generic scalars \\
 & $\alpha, \beta, \lambda$ & Parameters \\
 & $n, m, d$ & Dimensions \\
 & $y_i, \hat{y}_i$ & True/predicted values \\
\hline
\multirow{3}{*}{Vectors} & $\vx, \vy, \vz$ & Generic vectors \\
 & $\vw, \vtheta$ & Parameters \\
 & $\|\vx\|_p$ & Lp norm \\
\hline
\multirow{4}{*}{Matrices} & $\mX, \mA, \mB$ & Generic matrices \\
 & $\mI$ & Identity matrix \\
 & $\mSigma$ & Covariance matrix \\
 & $\text{rank}(\mA)$ & Matrix rank \\
\hline
\multirow{3}{*}{Spaces} & $\Real$ & Real numbers \\
 & $\Real^d$ & d-dimensional vectors \\
 & $\Real^{n \times d}$ & n×d matrices \\
\hline
\multirow{4}{*}{ML Metrics} & $\text{TP, FP, TN, FN}$ & Confusion matrix \\
 & $\text{Precision, Recall}$ & Classification metrics \\
 & $\MSE, \text{RMSE}$ & Regression metrics \\
 & $F_1$ & Harmonic mean of P \& R \\
\hline
\end{tabular}
\caption{Complete Mathematical Notation Reference}
\end{table}

\section{Further Reading}

\begin{itemize}
    \item \textbf{Linear Algebra:} Gilbert Strang, "Introduction to Linear Algebra"
    \item \textbf{ML Math:} Deisenroth, Faisal, Ong, "Mathematics for Machine Learning" 
    \item \textbf{Matrix Calculus:} Magnus \& Neudecker, "Matrix Differential Calculus with Applications"
    \item \textbf{Online Resources:} Khan Academy Linear Algebra, 3Blue1Brown Essence of Linear Algebra
    \item \textbf{NumPy Documentation:} \url{https://numpy.org/doc/stable/}
    \item \textbf{Scikit-learn Metrics:} \url{https://scikit-learn.org/stable/modules/model_evaluation.html}
\end{itemize}

\end{document}