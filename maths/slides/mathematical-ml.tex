\documentclass{beamer}
\usepackage{../../../shared/styles/custom}

%\beamerdefaultoverlayspecification{<+->}

%	\ifx\relax#1\relax  \item \else \item[#1] \fi
%	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}

\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}
\resetcounteronoverlays{saveenumi}

\title{Maths for ML}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle

% \section{Linear Regression}

\begin{frame}$$
	\epsilon = \begin{bmatrix}
	\epsilon_{1}   \\
	\epsilon_{2}   \\
	\dots \\
	\epsilon_{N}
	\end{bmatrix}_{N \times 1}   
	$$
	
	\pause $$
	\epsilon^T = \begin{bmatrix}
	\epsilon_{1}, 
	\epsilon_{2},  
	\dots, 
	\epsilon_{N}
	\end{bmatrix}_{1 \times N}   
	$$
	
	\pause $$\epsilon^T\epsilon = \sum \epsilon_{i}^{2}$$
	    \seti
	
\end{enumerate}

\end{frame}

\begin{frame}\item 
	For a scalar s
	$$
	s = s^{T}    
	$$
	\seti
\end{enumerate}

\end{frame}

\begin{frame}\begin{bmatrix}
  \frac{\partial s}{\partial \theta_{1}}\\
  \frac{\partial s}{\partial \theta_{2}}\\
    \vdots\\
  \frac{\partial s}{\partial \theta_{N}}\\
    \end{bmatrix}
 $$

\end{frame}

\begin{frame}\begin{equation*}
    X = \begin{bmatrix}
    a&b\\
    c&d
    \end{bmatrix}
\end{equation*}

\pause
\begin{equation*}
    X^{T} = \begin{bmatrix}
    a&c\\
    b&d
    \end{bmatrix}
\end{equation*}

\pause
\begin{equation*}
    Z = X^{T}X =  \begin{bmatrix}
    a^{2}+c^{2}&ab+cd\\
    ab+cd&b^{2}+d^{2}
    \end{bmatrix}_{2\times 2}
\end{equation*}

\pause
$Z$ has a property $Z_{ij}=Z_{ji} \implies Z^{T}=Z$

\end{frame}

\begin{frame}\begin{equation*}
        \theta = \begin{bmatrix}
        \theta_{1}\\
        \theta_{2}
        \end{bmatrix}_{2\times 1}
    \end{equation*}
    
    \pause
    \begin{equation*}
    \theta^{T}Z\theta=  \begin{bmatrix}
    \theta_1&\theta_2\\

    \end{bmatrix}_{1\times 2} \begin{bmatrix}
    e&f\\
    f&g
    \end{bmatrix}_{2\times 2}\begin{bmatrix}
    \theta_{1}\\
    \theta_{2}
    \end{bmatrix}_{2\times 1}
    \end{equation*}
    
    \pause
       \begin{equation*}
    \theta^{T}Z\theta=  \begin{bmatrix}
    \theta_1&\theta_2\\
    
    \end{bmatrix}_{1\times 2} \begin{bmatrix}
    e\theta_1+f\theta_2\\
    f\theta_1+g\theta_2
    \end{bmatrix}_{2\times 1}
    \end{equation*}

    \begin{equation*}
        \theta^{T}Z\theta = e\theta_{1}^{2} + 2f\theta_{1}\theta_{2}+g\theta_{2}^{2} 
    \end{equation*}

The term $\theta^{T}Z\theta$ is a scalar.

\end{frame}

\begin{frame}Since $X$ has fewer rows than columns, its maximum rank is equal to the maximum number of linearly independent rows. And because neither row is linearly dependent on the other row, the matrix has 2 linearly independent rows; so its rank is 2.
\end{frame}

\begin{frame}where $\mathrm{I}_{\mathrm{n}}$ is the identity matrix.

\pause  Below, with an example, we illustrate the relationship between a matrix and its inverse.

\pause \[
\begin{array}{l}
{\left[\begin{array}{cc}
	{2} & {1} \\
	{3} & {4}
	\end{array}\right]\left[\begin{array}{cc}
	{0.8} & {-0.2} \\
	{-0.6} & {0.4}
	\end{array}\right]=\left[\begin{array}{ll}
	{1} & {0} \\
	{0} & {1}
	\end{array}\right]} \\ \\
{\left[\begin{array}{cc}
	{0.8} & {-0.2} \\
	{-0.6} & {0.2}
	\end{array}\right]\left[\begin{array}{ll}
	{2} & {1} \\
	{3} & {4}
	\end{array}\right]=\left[\begin{array}{ll}
	{1} & {0} \\
	{0} & {1}
	\end{array}\right]}
\end{array}
\]

\end{frame}

\begin{frame}\begin{itemize}
\item If the rank of an $n \times n$ matrix is less than $n,$ the matrix does not have an inverse.
	\item When the determinant for a square matrix is equal to zero, the inverse for that matrix does not exist.
\end{itemize}
\pause A square matrix that has an inverse is said to be nonsingular or invertible; a square matrix that does not have an inverse is said to be singular. \\
\pause Not every square matrix has an inverse; but if a matrix does have an inverse, it is unique.

\end{frame}

\end{document}
