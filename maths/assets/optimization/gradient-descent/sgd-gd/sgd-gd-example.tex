\documentclass{beamer}
\usepackage{tcolorbox}
\usepackage{hyperref}


%\beamerdefaultoverlayspecification{<+->}
% \newcommand{\data}{\mathcal{D}}
% \newcommand\Item[1][]{%
% 	\ifx\relax#1\relax  \item \else \item[#1] \fi
% 	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}

\graphicspath{ {imgs/} }

\usetheme{metropolis}           % Use metropolis theme


\title{Gradient Descent and Stochastic Gradient Descent: Example}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
	\maketitle


	\begin{frame}{Stochastic Gradient Descent : Example}
		Learn $y = \theta_0 + \theta_1 x$ on following dataset, using SGD where initially $(\theta_0, \theta_1) = (4,0)$ and step-size, $\alpha  = 0.1$, for 1 epoch (3 iterations). 
		\begin{table}[]
		\centering
		\label{tab:my-table}
		\begin{tabular}{|c|c|}
		\hline
		\textbf{x} & \textbf{y} \\ \hline
		2 & 2 \\ \hline
		3 & 3 \\ \hline
		1 & 1 \\ \hline
		\end{tabular}
		\end{table}
	\end{frame}

	\begin{frame}{Stochastic Gradient Descent : Example}
		Our predictor, $\hat{y} = \theta_0 + \theta_1x$\\
		\vspace{1cm}
		Error for $i^{th}$ datapoint, $e_i = y_i - \hat{y_i}$\\
		$e_1 = 1 - \theta_0 - \theta_1$ \\
		$e_2 = 2 - \theta_0 - 2\theta_1$ \\
		$e_3 = 3 - \theta_0 - 3\theta_1$ \\
		
		\vspace{1cm}
		While using SGD, we compute the MSE using only 1 datapoint per iteration. \\
		So MSE is $e_1^2$ for iteration 1 and $e_2^2$ for iteration 2.
	\end{frame}


	\begin{frame}{Stochastic Gradient Descent : Example}
		\textbf{For Iteration $i$}\\
		\vspace{1cm}
		$\dfrac{\partial MSE}{\partial \theta_0} = 2\left( y_i - \theta_0 -\theta_1x_i \right)\left(-1\right) = 2e_i\left(-1\right)$ \\
		\vspace{2cm}
		$\dfrac{\partial MSE}{\partial \theta_1} = 2\left( y_i - \theta_0 -\theta_1x_i \right)\left(-x_i\right) = 2e_i\left(-x_i\right)$ 
	\end{frame}

	\begin{frame}{Stochastic Gradient Descent : Example}
		\textbf{Iteration 1}\\
		\vspace{0.5cm}
		$\theta_0 = \theta_0 - \alpha\dfrac{\partial MSE}{\partial \theta_0}$\\ 
		\vspace{0.5cm}
		\only<2->{
		$\theta_0 = 4 - 0.1 \times 2 \times \left( 2 - (4 + 0) \right)(-1)$\\
		
		\vspace{0.5cm}
		$\theta_0 = 3.6$
		\vspace{0.5cm}
		}
		
		$\theta_1 = \theta_1 - \alpha\dfrac{\partial MSE}{\partial \theta_1}$\\ 
		\vspace{0.5cm}
		\only<3->{
		$\theta_1 = 0 - 0.1 \times 2 \times \left( 2 - (4 + 0) \right)(-2)$\\
		\vspace{0.5cm}
		$\theta_1 = -0.8$
		}
	\end{frame}

	\begin{frame}{Stochastic Gradient Descent : Example}
		\textbf{Iteration 2}\\
		\vspace{0.5cm}
		$\theta_0 = \theta_0 - \alpha\dfrac{\partial MSE}{\partial \theta_0}$\\ 
		\vspace{0.5cm}
		\only<2->{
		$\theta_0 = 3.6 - 0.1 \times 2 \times \left( 3 - (3.6 - 0.8 \times 3 )\right)(-1) $\\
		
		\vspace{0.5cm}
		$\theta_0 = 3.96$
		\vspace{0.5cm}
		}
		
		$\theta_1 = \theta_1 - \alpha\dfrac{\partial MSE}{\partial \theta_1}$\\ 
		\vspace{0.5cm}
		\only<3->{
		$\theta_0 = -0.8 - 0.1 \times 2 \times \left( 3 - (3.6 - 0.8 \times 3 ) \right)(-3)$\\
		\vspace{0.5cm}
		$\theta_1 = 0.28$
		}
	\end{frame}

	\begin{frame}{Stochastic Gradient Descent : Example}
		\textbf{Iteration 3}\\
		\vspace{0.5cm}
		$\theta_0 = \theta_0 - \alpha\dfrac{\partial MSE}{\partial \theta_0}$\\ 
		\vspace{0.5cm}
		\only<2->{
		$\theta_0 = 3.96 - 0.1 \times 2 \times \left( 1 - (3.96 + 0.28 \times 1 )\right)(-1) $\\
		
		\vspace{0.5cm}
		$\theta_0 = 3.312$
		\vspace{0.5cm}
		}
		
		$\theta_1 = \theta_1 - \alpha\dfrac{\partial MSE}{\partial \theta_1}$\\ 
		\vspace{0.5cm}
		\only<3->{
		$\theta_0 = 0.28 - 0.1 \times 2 \times \left( 1 - (3.96 + 0.28 \times 1 )\right)(-1) $\\
		\vspace{0.5cm}
		$\theta_1 = -0.368$
		}
	\end{frame}

\end{document}