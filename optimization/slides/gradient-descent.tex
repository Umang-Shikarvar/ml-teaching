\documentclass[usenames,dvipsnames]{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}

\title{Gradient Descent}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle

  \section{Revision}
  \begin{frame}Gradient denotes the direction of steepest ascent or the direction in which there is a maximum increase in f(x,y) \\
\pause $\nabla f(x, y) = \begin{bmatrix}
\frac{\partial f(x, y)}{\partial x}\\
\frac{\partial f(x, y)}{\partial y}
\end{bmatrix} = \begin{bmatrix} 2x\\2y
\end{bmatrix}$

\end{frame}

  \section{Introduction}

  \begin{frame}Stochastic Gradient Descent
                        \begin{itemize}
\item In SGD, we update parameters after seeing each each point
                            \item Noisier curve for iteration vs cost 
                            \pause
\item For a single update, it computes the gradient over one example. Hence lesser time
                        \end{itemize}

                    \end{frame}
                    
                    \begin{frame}For $t$ iterations, what is the computational complexity of our gradient descent solution?

\pause Hint, rewrite the above as: \(\vtheta=\vtheta - \alpha \mX^{\top}\mX \vtheta+ \alpha \mX^{\top}\vy\) 

\pause Complexity of computing $\mX^{\top}\vy$ is $\mathcal{O}(dn)$

\pause Complexity of computing $\alpha \mX^{\top}\vy$ once we have $\mX^{\top}\vy$ is $\mathcal{O}(d)$ since $\mX^{\top}\vy$ has $d$ entries

\pause Complexity of computing $\mX^{\top}\mX$ is $\mathcal{O}(d^2n)$ and then multiplying with $\alpha$ is $\mathcal{O}(d^2)$

\pause All of the above need only be calculated once!

\end{frame}

\begin{frame}For each of the $t$ iterations, we now need to first multiply $\alpha \mX^{\top}\mX$ with $\vtheta$ which is matrix multiplication of a $d \times d$ matrix with a $d \times 1$, which is $\mathcal{O}(d^2)$ 

\pause The remaining subtraction/addition can be done in $\mathcal{O}(d)$ for each iteration.

\pause What is overall computational complexity?

\pause $\mathcal{O}(td^2)$ + $\mathcal{O}(d^2n) = \mathcal{O}((t+n)d^2)$
\end{frame}

\begin{frame}If we do not rewrite the expression
\(\vtheta=\vtheta - \alpha \mX^{\top}(\mX \vtheta-\vy)\) 

For each iteration, we have:
\begin{itemize}
\item Computing $\mX\vtheta$ is $\mathcal{O}(nd)$
	\item Computing $\mX\vtheta - \vy$ is $\mathcal{O}(n)$
	\pause
\item Computing $\alpha \mX^{\top}$ is $\mathcal{O}(nd)$
	\item Computing $\alpha \mX^{\top}(\mX\vtheta - \vy)$ is $\mathcal{O}(nd)$
	\item Computing \(\vtheta=\vtheta - \alpha \mX^{\top}(\mX \vtheta-\vy)\) is $\mathcal{O}(n)$
\end{itemize}

\pause What is overall computational complexity?

\pause $\mathcal{O}(ndt)$
\end{frame}

\end{document}
