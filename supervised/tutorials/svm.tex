\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage[bookmarks=false]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}

\pagestyle{fancy}
\fancyhf{}
\rhead{Tutorial: Support Vector Machines}
\lhead{ES335 - Machine Learning}
\rfoot{Page \thepage}

\titleformat{\section}{\Large\bfseries\color{blue!75!black}}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\color{blue!60!black}}{\thesubsection}{1em}{}

\usepackage{../../shared/styles/conventions}
\usepackage{../../shared/notation/notation}

\newcounter{exercise}
\setcounter{exercise}{0}

\title{\textbf{Tutorial: Support Vector Machines} \\ \textit{Cheat Sheet and Practice Problems}}
\author{ES335 - Machine Learning \\ IIT Gandhinagar}
\date{\today}

\begin{document}

\maketitle

\section{Summary from Slides}

\subsection{Key Concepts}

\textbf{What is SVM?}
\begin{itemize}
    \item Binary classification algorithm that finds optimal separating hyperplane
    \item Maximizes margin between classes for better generalization
    \item Uses support vectors (closest points to decision boundary)
    \item Can handle non-linearly separable data using kernel trick
\end{itemize}

\textbf{Core Idea: Margin Maximization}
\begin{itemize}
    \item Margin = perpendicular distance between two parallel hyperplanes
    \item Distance between hyperplanes $\vw \cdot \vx + b_1 = 0$ and $\vw \cdot \vx + b_2 = 0$ is $\frac{|b_1 - b_2|}{\norm{\vw}}$
    \item Maximum margin classifier: $\text{margin} = \frac{2}{\norm{\vw}}$
\end{itemize}

\subsection{Hard Margin SVM (Linearly Separable)}

\textbf{Primal Formulation:}
\begin{align}
\minimize & \quad \frac{1}{2}\norm{\vw}^2 \\
\subjectto & \quad y_i(\vw \cdot \vx_i + b) \geq 1 \quad \forall i
\end{align}

\textbf{Dual Formulation:}
\begin{align}
\maximize & \quad L(\valpha) = \sum_{i=1}^{N} \alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j \vx_i \cdot \vx_j \\
\subjectto & \quad \sum_{i=1}^{N} \alpha_i y_i = 0, \quad \alpha_i \geq 0
\end{align}

\textbf{KKT Conditions:}
\begin{itemize}
    \item $\alpha_i (y_i(\vw \cdot \vx_i + b) - 1) = 0$ (complementary slackness)
    \item Support vectors: $\alpha_i \neq 0$ where $y_i(\vw \cdot \vx_i + b) = 1$
    \item Non-support vectors: $\alpha_i = 0$ where $y_i(\vw \cdot \vx_i + b) > 1$
\end{itemize}

\subsection{Kernel Methods}

\textbf{Kernel Trick:}
\begin{itemize}
    \item Transform data to higher dimensional space using $\phi: \mathbb{R}^d \rightarrow \mathbb{R}^D$
    \item Compute kernel function $K(\vx_i, \vx_j) = \phi(\vx_i) \cdot \phi(\vx_j)$ efficiently
    \item Avoid explicit computation of $\phi(\vx)$
\end{itemize}

\textbf{Common Kernels:}
\begin{itemize}
    \item Linear: $K(\vx_i, \vx_j) = \vx_i \cdot \vx_j$
    \item Polynomial: $K(\vx_i, \vx_j) = (c + \vx_i \cdot \vx_j)^d$
    \item RBF (Gaussian): $K(\vx_i, \vx_j) = \exp(-\gamma \norm{\vx_i - \vx_j}^2)$
\end{itemize}

\textbf{Properties:}
\begin{itemize}
    \item RBF kernel corresponds to infinite-dimensional feature space
    \item RBF SVM is non-parametric (model complexity grows with data)
    \item Linear/polynomial kernels are parametric
\end{itemize}

\subsection{Soft Margin SVM}

\textbf{Motivation:} Handle non-separable data with noise and outliers

\textbf{Primal Formulation:}
\begin{align}
\minimize & \quad \frac{1}{2}\norm{\vw}^2 + C \sum_{i=1}^{N} \xi_i \\
\subjectto & \quad y_i(\vw \cdot \vx_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\end{align}

\textbf{Dual Formulation:}
\begin{align}
\maximize & \quad \sum_{i=1}^{N} \alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j \vx_i \cdot \vx_j \\
\subjectto & \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^{N} \alpha_i y_i = 0
\end{align}

\textbf{Hinge Loss Formulation:}
$$\minimize \sum_{i=1}^{N} \max[0, 1 - y_i(\vw \cdot \vx_i + b)] + \frac{1}{2C}\norm{\vw}^2$$

\textbf{Parameter C:}
\begin{itemize}
    \item Large C: Less tolerance for violations (high variance, low bias)
    \item Small C: More tolerance for violations (low variance, high bias)
    \item $C \rightarrow \infty$: Hard margin SVM
\end{itemize}

\section{Practice Problems}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Basic SVM Concepts]
Consider a 1D dataset with points: $(1, +1), (2, +1), (-1, -1), (-2, -1)$.

\textbf{Part A:} What is the optimal separating hyperplane?

\textbf{Part B:} Calculate the margin of this hyperplane.

\textbf{Part C:} Which points are the support vectors?

\textbf{Part D:} What are the values of $\alpha_i$ for each point?
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Distance Between Hyperplanes]
Given two parallel hyperplanes:
\begin{itemize}
    \item $\vw \cdot \vx + 5 = 0$
    \item $\vw \cdot \vx - 3 = 0$
\end{itemize}
where $\norm{\vw} = 4$.

\textbf{Part A:} Calculate the distance between these hyperplanes.

\textbf{Part B:} If this represents the margin in an SVM, what is $\norm{\vw}$ for the decision boundary $\vw \cdot \vx + 1 = 0$?
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: KKT Conditions]
For an SVM with solution $\vw = [1, -1]^T$, $b = 0$, consider the following points:
\begin{itemize}
    \item Point A: $\vx = [2, 1]^T$, $y = +1$, $\alpha = 0.5$
    \item Point B: $\vx = [0, 1]^T$, $y = +1$, $\alpha = 0$
    \item Point C: $\vx = [-1, 0]^T$, $y = -1$, $\alpha = 0.5$
\end{itemize}

\textbf{Part A:} Verify the KKT complementary slackness condition for each point.

\textbf{Part B:} Which points are support vectors?

\textbf{Part C:} Is the constraint $\sum_i \alpha_i y_i = 0$ satisfied?
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Dual Problem Setup]
Consider a binary classification problem with 3 training points:
\begin{itemize}
    \item $(\vx_1, y_1) = ([1, 0]^T, +1)$
    \item $(\vx_2, y_2) = ([0, 1]^T, +1)$
    \item $(\vx_3, y_3) = ([-1, -1]^T, -1)$
\end{itemize}

\textbf{Part A:} Write the dual objective function $L(\valpha)$ explicitly.

\textbf{Part B:} What are the constraints on $\alpha_1, \alpha_2, \alpha_3$?

\textbf{Part C:} If the optimal solution is $\alpha_1 = 0.5, \alpha_2 = 0.5, \alpha_3 = 1$, find $\vw$.
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Kernel Computation]
Consider the polynomial kernel $K(\vx, \vz) = (1 + \vx \cdot \vz)^2$ for $\vx, \vz \in \mathbb{R}^2$.

\textbf{Part A:} For $\vx = [2, 1]^T$ and $\vz = [1, 3]^T$, compute $K(\vx, \vz)$.

\textbf{Part B:} Find the explicit feature mapping $\phi(\vx)$ such that $K(\vx, \vz) = \phi(\vx) \cdot \phi(\vz)$.

\textbf{Part C:} What is the dimensionality of the feature space?

\textbf{Part D:} Verify your answer by computing $\phi(\vx) \cdot \phi(\vz)$ directly.
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: RBF Kernel Properties]
Consider the RBF kernel $K(\vx, \vz) = \exp(-\gamma \norm{\vx - \vz}^2)$ with $\gamma = 0.5$.

\textbf{Part A:} Compute $K([0, 0]^T, [1, 1]^T)$.

\textbf{Part B:} What happens to $K(\vx, \vz)$ as $\norm{\vx - \vz} \rightarrow \infty$?

\textbf{Part C:} What happens to $K(\vx, \vz)$ as $\norm{\vx - \vz} \rightarrow 0$?

\textbf{Part D:} How does increasing $\gamma$ affect the kernel's behavior?

\textbf{Part E:} Why is RBF SVM considered non-parametric?
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Soft Margin Formulation]
Consider a soft margin SVM with regularization parameter $C = 2$.

\textbf{Part A:} Write the primal optimization problem.

\textbf{Part B:} Express the problem in hinge loss form.

\textbf{Part C:} What are the dual constraints?

\textbf{Part D:} If a training point has $\xi_i = 1.5$, what does this mean for the point's classification?
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Support Vector Analysis]
In a soft margin SVM, classify the following scenarios for training points:

\textbf{Part A:} Point with $y_i(\vw \cdot \vx_i + b) = 1.5$ and $\alpha_i = 0$

\textbf{Part B:} Point with $y_i(\vw \cdot \vx_i + b) = 1$ and $\alpha_i = 0.3$

\textbf{Part C:} Point with $y_i(\vw \cdot \vx_i + b) = 0.2$ and $\alpha_i = C$

\textbf{Part D:} Point with $y_i(\vw \cdot \vx_i + b) = -0.5$ and $\alpha_i = C$

For each case, determine: (i) if it's a support vector, (ii) if it's correctly classified, (iii) the value of $\xi_i$.
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Prediction with Kernels]
Given an SVM with RBF kernel $K(\vx, \vz) = \exp(-\norm{\vx - \vz}^2)$ and the following support vectors:
\begin{itemize}
    \item $\vx_1 = [1, 0]^T$, $y_1 = +1$, $\alpha_1 = 0.5$
    \item $\vx_2 = [-1, 0]^T$, $y_2 = -1$, $\alpha_2 = 0.5$
\end{itemize}
with $b = 0$.

\textbf{Part A:} Write the decision function $f(\vx)$.

\textbf{Part B:} Predict the class for test point $\vx_{test} = [0.5, 0]^T$.

\textbf{Part C:} Predict the class for test point $\vx_{test} = [0, 1]^T$.

\textbf{Part D:} At what point would the decision function output exactly 0?
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Multi-class SVM]
You have a 3-class problem with classes A, B, and C. Using the one-vs-all approach:

\textbf{Part A:} How many binary classifiers do you need?

\textbf{Part B:} For a test point, the classifiers output:
\begin{itemize}
    \item A vs (B,C): $f_A(\vx) = 0.8$
    \item B vs (A,C): $f_B(\vx) = 0.6$
    \item C vs (A,B): $f_C(\vx) = -0.2$
\end{itemize}
Which class should be predicted?

\textbf{Part C:} What could be a problem with this approach?

\textbf{Part D:} How many classifiers would one-vs-one approach require?
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Regularization Parameter C]
Consider the effect of parameter C in soft margin SVM:

\textbf{Part A:} What happens to the decision boundary as $C \rightarrow 0$?

\textbf{Part B:} What happens to the decision boundary as $C \rightarrow \infty$?

\textbf{Part C:} Given a noisy dataset, should you use high or low C? Justify your answer.

\textbf{Part D:} How does C affect the bias-variance tradeoff?

\textbf{Part E:} If you have 100 training points and C = 10, what's the maximum possible value of $\sum_{i=1}^{100} \xi_i$?
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Hinge Loss Analysis]
The hinge loss is defined as $\ell(y, f(\vx)) = \max[0, 1 - y \cdot f(\vx)]$.

\textbf{Part A:} For a correctly classified point with $y \cdot f(\vx) = 2$, what is the hinge loss?

\textbf{Part B:} For a point on the margin with $y \cdot f(\vx) = 1$, what is the hinge loss?

\textbf{Part C:} For a misclassified point with $y \cdot f(\vx) = -0.5$, what is the hinge loss?

\textbf{Part D:} At what value of $y \cdot f(\vx)$ is the hinge loss non-differentiable?

\textbf{Part E:} Compare hinge loss with 0-1 loss and logistic loss for $y \cdot f(\vx) \in [-2, 3]$.
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Computational Complexity]
Consider the computational aspects of SVM:

\textbf{Part A:} Why is the dual formulation preferred for implementing the kernel trick?

\textbf{Part B:} In the dual problem, what is the dominant computational cost?

\textbf{Part C:} For a dataset with n points and d features, compare the computational complexity of:
\begin{itemize}
    \item Linear kernel SVM
    \item RBF kernel SVM
    \item Explicit feature mapping with degree-2 polynomial kernel
\end{itemize}

\textbf{Part D:} How does the number of support vectors affect prediction time?

\textbf{Part E:} Why might you prefer linear SVM over RBF SVM for very large datasets?
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Advanced Kernel Design]
Design custom kernels for specific scenarios:

\textbf{Part A:} Prove that $K(\vx, \vz) = K_1(\vx, \vz) + K_2(\vx, \vz)$ is a valid kernel if $K_1$ and $K_2$ are valid kernels.

\textbf{Part B:} Prove that $K(\vx, \vz) = K_1(\vx, \vz) \cdot K_2(\vx, \vz)$ is a valid kernel if $K_1$ and $K_2$ are valid kernels.

\textbf{Part C:} Design a kernel for text classification where documents are represented as word frequency vectors.

\textbf{Part D:} For time series data, propose a kernel that captures both magnitude and temporal patterns.

\textbf{Part E:} What properties must a function $K(\vx, \vz)$ satisfy to be a valid kernel (Mercer's theorem)?
\end{tcolorbox}

\stepcounter{exercise}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Exercise \theexercise: Comprehensive SVM Problem]
You are given a 2D dataset that forms two concentric circles (inner circle: class +1, outer circle: class -1).

\textbf{Part A:} Why would linear SVM fail on this dataset?

\textbf{Part B:} Propose a feature transformation $\phi(\vx)$ that could make the data linearly separable.

\textbf{Part C:} Design a custom kernel that directly computes the dot product in your transformed space.

\textbf{Part D:} How would you choose between polynomial and RBF kernels for this problem?

\textbf{Part E:} If there are outliers in the inner circle that are very close to the decision boundary, how would you handle them?

\textbf{Part F:} Describe a complete pipeline for solving this problem, including data preprocessing, model selection, and evaluation.
\end{tcolorbox}

\section{Key Takeaways}

\begin{itemize}
    \item SVM finds the optimal separating hyperplane by maximizing the margin
    \item Dual formulation enables the kernel trick for non-linear classification
    \item Support vectors are the critical points that define the decision boundary
    \item Soft margin allows handling of non-separable data with controlled violations
    \item Parameter C controls the bias-variance tradeoff in soft margin SVM
    \item Kernel choice significantly impacts model performance and interpretability
    \item Hinge loss provides a convex surrogate for 0-1 loss
    \item SVM is both theoretically grounded and practically effective
\end{itemize}

\end{document}