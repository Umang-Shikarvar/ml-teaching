\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage[bookmarks=false]{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes.geometric}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{titlesec}

\pagestyle{fancy}
\fancyhf{}
\rhead{Tutorial: Naive Bayes}
\lhead{ES335 - Machine Learning}
\rfoot{Page \thepage}

\titleformat{\section}{\Large\bfseries\color{blue!75!black}}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\color{blue!60!black}}{\thesubsection}{1em}{}

\usepackage{../../shared/styles/conventions}

\newcounter{exercise}
\setcounter{exercise}{0}

\title{\textbf{Tutorial: Naive Bayes} \\ \textit{Cheat Sheet and Practice Problems}}
\author{ES335 - Machine Learning \\ IIT Gandhinagar}
\date{\today}

\begin{document}

\maketitle

\section{Summary from Slides}

\subsection{Bayes' Theorem Foundation}

\textbf{Bayes' Theorem}:
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

\textbf{For Machine Learning Classification}:
$$P(y|x_1, x_2, \ldots, x_n) = \frac{P(x_1, x_2, \ldots, x_n|y)P(y)}{P(x_1, x_2, \ldots, x_n)}$$

Where:
\begin{itemize}
    \item $P(y|x_1, x_2, \ldots, x_n)$: Posterior probability (what we want to predict)
    \item $P(x_1, x_2, \ldots, x_n|y)$: Likelihood of features given class
    \item $P(y)$: Prior probability of class
    \item $P(x_1, x_2, \ldots, x_n)$: Evidence (normalizing constant)
\end{itemize}

\subsection{The Naive Assumption}

\textbf{Why ``Naive''?} Assumes features are conditionally independent given the class:
$$P(x_1, x_2, \ldots, x_n|y) = P(x_1|y) \cdot P(x_2|y) \cdots P(x_n|y) = \prod_{i=1}^{n} P(x_i|y)$$

This simplifies the model dramatically but is often violated in practice.

\subsection{Naive Bayes Classification Rule}

\textbf{Prediction}: Choose class with highest posterior probability
$$\hat{y} = \arg\max_{y} P(y|x_1, x_2, \ldots, x_n) = \arg\max_{y} P(y)\prod_{i=1}^{n} P(x_i|y)$$

Since denominator $P(x_1, x_2, \ldots, x_n)$ is constant across classes, we can ignore it.

\subsection{Types of Naive Bayes}

\textbf{1. Categorical/Multinomial Naive Bayes} (for discrete features):
$$P(x_i = k|y = c) = \frac{\text{Count}(x_i = k \text{ and } y = c)}{\text{Count}(y = c)}$$

\textbf{2. Gaussian Naive Bayes} (for continuous features):
$$P(x_i = v|y = c) = \frac{1}{\sqrt{2\pi\sigma_{c}^2}} \exp\left(-\frac{(v-\mu_c)^2}{2\sigma_c^2}\right)$$

Where $\mu_c$ and $\sigma_c^2$ are the mean and variance of feature $x_i$ for class $c$.

\subsection{Parameter Estimation}

\textbf{Prior Probabilities}:
$$P(y = c) = \frac{\text{Count}(y = c)}{N}$$

\textbf{Likelihood for Categorical Features}:
$$P(x_i = k|y = c) = \frac{\text{Count}(x_i = k, y = c)}{\text{Count}(y = c)}$$

\textbf{Likelihood for Gaussian Features}:
$$\mu_c = \frac{1}{N_c} \sum_{i: y_i = c} x_i, \quad \sigma_c^2 = \frac{1}{N_c} \sum_{i: y_i = c} (x_i - \mu_c)^2$$

\subsection{Laplace Smoothing}

\textbf{Problem}: Zero probabilities when feature values not seen in training data.

\textbf{Solution}: Add pseudocounts ($\alpha = 1$ for Laplace smoothing):
$$P(x_i = k|y = c) = \frac{\text{Count}(x_i = k, y = c) + \alpha}{\text{Count}(y = c) + \alpha \cdot |\text{vocabulary}|}$$

\section{Practice Problems}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Basic Bayes' Theorem]

A medical test for a rare disease has the following characteristics:
\begin{itemize}
    \item $P(\text{Test} = +|\text{Disease} = \text{True}) = 0.99$
    \item $P(\text{Test} = -|\text{Disease} = \text{False}) = 0.99$
    \item $P(\text{Disease} = \text{True}) = 0.0001$
\end{itemize}

If someone tests positive, what is the probability they actually have the disease?
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Spam Email Classification - Setup]

Given the following email dataset with binary features (word present = 1, absent = 0):

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Email & ``free'' & ``money'' & Class \\
\hline
1 & 1 & 0 & Spam \\
2 & 1 & 1 & Spam \\
3 & 0 & 1 & Spam \\
4 & 0 & 0 & Ham \\
5 & 1 & 0 & Ham \\
6 & 0 & 0 & Ham \\
\hline
\end{tabular}
\end{center}

Calculate all prior probabilities $P(\text{Spam})$ and $P(\text{Ham})$.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Likelihood Calculation]

Using the spam email dataset from Problem 2, calculate:
\begin{itemize}
    \item $P(\text{"free"} = 1|\text{Spam})$
    \item $P(\text{"free"} = 0|\text{Spam})$
    \item $P(\text{"money"} = 1|\text{Ham})$
    \item $P(\text{"money"} = 0|\text{Ham})$
\end{itemize}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Classification Decision]

Using the spam email dataset, classify a new email with features [``free'' = 1, ``money'' = 0]. Show all calculations and determine whether it's Spam or Ham.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Gaussian Naive Bayes Setup]

For the height/weight/gender classification problem:

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Height & Weight & Foot Size & Gender \\
\hline
6.0 & 180 & 12 & M \\
5.92 & 190 & 11 & M \\
5.58 & 170 & 12 & M \\
5.92 & 165 & 10 & M \\
5.0 & 100 & 6 & F \\
5.5 & 150 & 8 & F \\
5.42 & 130 & 7 & F \\
5.75 & 150 & 9 & F \\
\hline
\end{tabular}
\end{center}

Calculate the mean and variance for height for both Male and Female classes.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Gaussian Probability Calculation]

Using the Gaussian parameters from Problem 5, calculate $P(\text{Height} = 6.0|\text{Male})$ using the Gaussian probability density function.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Complete Gaussian Classification]

Classify a person with Height = 6.0, Weight = 130, Foot Size = 8 using Gaussian Naive Bayes. Calculate the posterior probabilities for both Male and Female classes.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Laplace Smoothing]

Consider a text classification problem with vocabulary = [``good'', ``bad'', ``movie'']:

Training data:
\begin{itemize}
    \item Positive: [``good movie''], [``good'']
    \item Negative: [``bad movie''], [``bad'']
\end{itemize}

Calculate $P(\text{"excellent"}|\text{Positive})$ with and without Laplace smoothing ($\alpha = 1$).
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Independence Assumption Analysis]

Consider features: ``Age'' and ``Income'' for credit approval.
\begin{itemize}
    \item Real correlation between Age and Income: 0.7
    \item Naive Bayes assumes: correlation = 0
\end{itemize}

Explain how this independence assumption might affect:
a) Model accuracy
b) Feature importance interpretation
c) When the model might still work well despite violated assumptions
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Multinomial Naive Bayes]

For document classification, you have word counts:

Document 1 (Sports): ``game'': 3, ``team'': 2, ``player'': 1
Document 2 (Politics): ``government'': 2, ``policy'': 3, ``vote'': 1

Calculate the probability of the word ``team'' given the Sports class using multinomial distribution parameters.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Log-Space Computation]

When multiplying many small probabilities, we use log-space to avoid numerical underflow.

Transform this calculation to log-space:
$$P(\text{Class}|\text{features}) = P(\text{Class}) \times P(f_1|\text{Class}) \times P(f_2|\text{Class}) \times P(f_3|\text{Class})$$

Given: $P(\text{Class}) = 0.3$, $P(f_1|\text{Class}) = 0.1$, $P(f_2|\text{Class}) = 0.05$, $P(f_3|\text{Class}) = 0.02$
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Handling Missing Features]

In the spam classification dataset, suppose a new email has:
\begin{itemize}
    \item ``free'': present
    \item ``money'': missing (unknown)
    \item ``urgent'': not in original vocabulary
\end{itemize}

Describe three strategies for handling missing and unknown features in Naive Bayes classification.
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Model Evaluation]

You trained a Naive Bayes classifier with the following confusion matrix:

\begin{center}
\begin{tabular}{c|cc}
 & Predicted: + & Predicted: - \\
\hline
Actual: + & 80 & 20 \\
Actual: - & 10 & 90 \\
\end{tabular}
\end{center}

Calculate:
a) Accuracy
b) Precision for positive class
c) Recall for positive class
d) F1-score
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Feature Selection Impact]

You have 1000 features for text classification. When you:
\begin{itemize}
    \item Use all 1000 features: Accuracy = 85\%
    \item Use top 100 features (by mutual information): Accuracy = 88\%
    \item Use top 50 features: Accuracy = 84\%
\end{itemize}

Explain why reducing features can improve Naive Bayes performance. What does this suggest about the independence assumption?
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Problem \stepcounter{exercise}: Real-world Application Design]

Design a Naive Bayes system for movie review sentiment analysis:

\textbf{Dataset}: 10,000 movie reviews (positive/negative)
\textbf{Features}: Word frequencies, review length, rating

a) Choose appropriate Naive Bayes variant for each feature type
b) Describe preprocessing steps
c) How would you handle the class imbalance (70\% positive, 30\% negative)?
d) Design evaluation methodology
e) Identify potential limitations and when Naive Bayes might fail
\end{tcolorbox}

\end{document}