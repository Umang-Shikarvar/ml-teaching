\documentclass[usenames,dvipsnames]{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}



\title{Decision Trees}
\date{\today}
\author{Nipun Batra and teaching staff}
\institute{IIT Gandhinagar}

% Pop quiz counter is defined globally in theme-nipun.sty
\setcounter{popquiz}{0}

\begin{document}
	\maketitle
	
	% Table of Contents
	\begin{frame}{Table of Contents}
		\tableofcontents[hideallsubsections]
	\end{frame}
	
	\section{Introduction and Motivation}
	
\begin{frame}{The need for interpretability}
	\begin{figure}
		\centering
		\includegraphics[scale=0.32]{../assets/decision-trees/diagrams/interpretability}
		\label{fig:interpretability}
	\end{figure}
	
\end{frame}
	
	\renewcommand{\arraystretch}{0.85}
	\begin{frame}{Training Data}
	\begin{tabular}{lllll||l} \toprule
	\textbf{Day} & \textbf{Outlook}  & \textbf{Temp} & \textbf{Humidity} & \textbf{Windy}  & \textbf{Play} \\ \midrule
	D1  & Sunny    & Hot  & High     & Weak   & No   \\
	D2  & Sunny    & Hot  & High     & Strong & No   \\
	D3  & Overcast & Hot  & High     & Weak   & Yes  \\
	D4  & Rain     & Mild & High     & Weak   & Yes  \\
	D5  & Rain     & Cool & Normal   & Weak   & Yes  \\
	D6  & Rain     & Cool & Normal   & Strong & No   \\
	D7  & Overcast & Cool & Normal   & Strong & Yes  \\
	D8  & Sunny    & Mild & High     & Weak   & No   \\
	D9  & Sunny    & Cool & Normal   & Weak   & Yes  \\
	D10 & Rain     & Mild & Normal   & Weak   & Yes  \\
	D11 & Sunny    & Mild & Normal   & Strong & Yes  \\
	D12 & Overcast & Mild & High     & Strong & Yes  \\
	D13 & Overcast & Hot  & Normal   & Weak   & Yes  \\
	D14 & Rain     & Mild & High     & Strong & No  \\ \bottomrule
	\end{tabular}
\end{frame}

\begin{frame}{Learning a Complicated Neural Network}
\begin{center}
\begin{tikzpicture}[
	node distance=1.5cm and 2cm,
	neuron/.style={circle, draw, minimum size=0.8cm, inner sep=0pt, fill=blue!20},
	input/.style={circle, draw, minimum size=0.8cm, inner sep=0pt, fill=green!20},
	output/.style={circle, draw, minimum size=0.8cm, inner sep=0pt, fill=red!20},
	connection/.style={->, >=stealth, thick, opacity=0.7}
	]
	
	% Input layer (6 nodes)
	\foreach \y in {1,2,3,4,5,6}
		\node[input] (i\y) at (0, 6-\y) {$x_\y$};
	
	% Hidden layer 1 (5 nodes)
	\foreach \y in {1,2,3,4,5}
		\node[neuron] (h1\y) at (2.5, 5.5-\y) {};
	
	% Hidden layer 2 (5 nodes) 
	\foreach \y in {1,2,3,4,5}
		\node[neuron] (h2\y) at (5, 5.5-\y) {};
	
	% Hidden layer 3 (4 nodes)
	\foreach \y in {1,2,3,4}
		\node[neuron] (h3\y) at (7.5, 5-\y) {};
	
	% Output layer (3 nodes)
	\foreach \y in {1,2,3}
		\node[output] (o\y) at (10, 4.5-\y) {$\hat{y}_\y$};
	
	% Connections: Input to Hidden1
	\foreach \i in {1,2,3,4,5,6} {
		\foreach \j in {1,2,3,4,5} {
			\draw[connection] (i\i) -- (h1\j);
		}
	}
	
	% Connections: Hidden1 to Hidden2
	\foreach \i in {1,2,3,4,5} {
		\foreach \j in {1,2,3,4,5} {
			\draw[connection] (h1\i) -- (h2\j);
		}
	}
	
	% Connections: Hidden2 to Hidden3
	\foreach \i in {1,2,3,4,5} {
		\foreach \j in {1,2,3,4} {
			\draw[connection] (h2\i) -- (h3\j);
		}
	}
	
	% Connections: Hidden3 to Output
	\foreach \i in {1,2,3,4} {
		\foreach \j in {1,2,3} {
			\draw[connection] (h3\i) -- (o\j);
		}
	}
	
	% Layer labels
	\node[above] at (0, 1.5) {\textbf{Input}};
	\node[above] at (2.5, 1.5) {\textbf{Hidden 1}};
	\node[above] at (5, 1.5) {\textbf{Hidden 2}};
	\node[above] at (7.5, 1.5) {\textbf{Hidden 3}};
	\node[above] at (10, 1.5) {\textbf{Output}};
	
\end{tikzpicture}
\end{center}

%\textbf{Complex model, same evaluation:} Even with this intricate network, we still use the same accuracy metrics to evaluate performance!
\end{frame}

\begin{frame}{Learnt Decision Tree}
\begin{tikzpicture}[
node/.style={%
	draw,
	rectangle,
},
]

\node [node] (A) {Outlook};
\path (A) ++(-150:\nodeDist) node [node] (B) {Humidity};
\path (A) ++(-90:\nodeDist/2) node [node, fill=green] (C) {Yes};
\path (A) ++(-30:\nodeDist) node [node] (D) {Wind};
\path (B) ++(-135:\nodeDist) node [node, fill=red] (E) {No};
\path (B) ++(-45:\nodeDist) node [node, fill=green] (F) {Yes};
\path (D) ++(-45:\nodeDist) node [node, fill=red] (G) {No};
\path (D) ++(-135:\nodeDist) node [node, fill=green] (H) {Yes};

\draw (A) -- (B) node [left,pos=0.25] {Sunny}(A);
\draw (A) -- (C) node [right,pos=0.8] {Overcast}(A);
\draw (A) -- (D) node [right,pos=0.5] {Rain}(A);
\draw (B) -- (E) node [left,pos=0.25] {High}(A);
\draw (B) -- (F) node [right,pos=0.25] {Normal}(C);
\draw (D) -- (G) node [right,pos=0.25] {Strong}(A);
\draw (D) -- (H) node [left,pos=0.25] {Weak}(A);
\end{tikzpicture}

\end{frame}

\begin{frame}{Medical Diagnosis using Decision Trees }
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{../assets/decision-trees/diagrams/decision-medical}
	\caption{Source: Improving medical decision trees by combining relevant health-care criteria}
	\label{fig:decision-medical}
\end{figure}

\end{frame}

\begin{frame}{Leo Breiman (1928-2005)}
\begin{center}
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{../assets/decision-trees/diagrams/brieman}
	\label{fig:brieman}
\end{figure}
\end{center}
\end{frame}

\begin{frame}{Leo Breiman: Revolutionary Contributions to ML}
\begin{keypointsbox}
\textbf{Major Algorithmic Breakthroughs:}
\begin{itemize}
\item \textbf{CART (1984)}: Classification and Regression Trees
\item \textbf{Bagging (1994)}: Bootstrap Aggregating
\item \textbf{Random Forests (2001)}: Ensemble of decision trees
\item \textbf{Two Cultures (2001)}: Data modeling vs. algorithmic modeling
\end{itemize}
\end{keypointsbox}
\end{frame}


\begin{frame}{Computational Complexity Classes: A Quick Primer}
\begin{definitionbox}{Key Complexity Classes}
\begin{itemize}
\item \textbf{P}: Problems solvable in polynomial time
\begin{itemize}
\item Example: Sorting $n$ numbers in $O(n \log n)$ time
\end{itemize}
\item \textbf{NP}: Problems where solutions can be \emph{verified} in polynomial time
\begin{itemize}
\item Example: Given a sudoku solution, verify it's correct
\end{itemize}
\item \textbf{NP-Complete}: Hardest problems in NP 
\begin{itemize}
\item Both in NP and at least as hard as any NP problem
\item Example: Boolean satisfiability (SAT)
\end{itemize}
\item \textbf{NP-Hard}: At least as hard as NP-Complete problems
\begin{itemize}
\item May not be in NP (solutions might not be verifiable quickly)
\item Example: Optimization versions of NP-Complete problems
\end{itemize}
\end{itemize}
\end{definitionbox}
\end{frame}




\begin{frame}{Finding the Optimal Decision Tree}
\begin{center}
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{../assets/decision-trees/diagrams/NP-hard}
	\label{fig:np-hard}
\end{figure}
\end{center}
\textbf{The Problem}: Given training data, find the decision tree with highest accuracy
\end{frame}

\begin{frame}{Optimal Decision Trees are NP-Complete}
\begin{alertbox}{Computational Complexity}
\textbf{Finding optimal decision tree is NP-Complete}
\begin{itemize}
\item \textbf{Verification}: Given a tree, check its accuracy quickly \checkmark
\item \textbf{Construction}: Exponentially many trees to check \crossmark
\end{itemize}
\end{alertbox}

\pause
\begin{examplebox}{What This Means}
\begin{itemize}
\item No efficient algorithm exists (unless P = NP)
\item Must use heuristics like greedy algorithms
\item ID3, C4.5, CART use greedy approaches
\item Good solutions, but no optimality guarantee
\end{itemize}
\end{examplebox}
\end{frame}


\begin{frame}{Greedy Algorithm}
Core idea: At each level, choose an attribute that gives
\textbf{biggest estimated} performance gain!

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{../assets/decision-trees/diagrams/gredy}
	\caption{Greedy $\neq$ Optimal}
	\label{fig:gredy}
\end{figure}

\end{frame}


\section{Discrete Input, Discrete Output}



\begin{frame}{Towards biggest estimated performance gain}
\begin{columns}
\begin{column}{.7\textwidth}


\begin{scriptsize}


	\begin{tabular}{lllll||l} \toprule
	\textbf{Day} & \textbf{Outlook}  & \textbf{Temp} & \textbf{Humidity} & \textbf{Windy}  & \textbf{Play} \\ \midrule
	D1  & Sunny    & Hot  & High     & Weak   & No   \\
	D2  & Sunny    & Hot  & High     & Strong & No   \\
	D3  & Overcast & Hot  & High     & Weak   & Yes  \\
	D4  & Rain     & Mild & High     & Weak   & Yes  \\
	D5  & Rain     & Cool & Normal   & Weak   & Yes  \\
	D6  & Rain     & Cool & Normal   & Strong & No   \\
	D7  & Overcast & Cool & Normal   & Strong & Yes  \\
	D8  & Sunny    & Mild & High     & Weak   & No   \\
	D9  & Sunny    & Cool & Normal   & Weak   & Yes  \\
	D10 & Rain     & Mild & Normal   & Weak   & Yes  \\
	D11 & Sunny    & Mild & Normal   & Strong & Yes  \\
	D12 & Overcast & Mild & High     & Strong & Yes  \\
	D13 & Overcast & Hot  & Normal   & Weak   & Yes  \\
	D14 & Rain     & Mild & High     & Strong & No  \\ \bottomrule
\end{tabular}
\end{scriptsize}
\end{column}
\begin{column}{0.4\textwidth}
	\begin{scriptsize}
\begin{itemize}
	\pause \item For examples, we have 9 Yes, 5 No
	\pause \item Would it be trivial if we had 14 Yes or 14 No?
	\pause \item Yes!
	\pause \item Key insight: Problem is ``easier'' when there is less disagreement
	\pause 	\item Need some statistical measure of ``disagreement'' 
	\end{itemize}

\end{scriptsize}
\end{column}

\end{columns}

\end{frame}




%\begin{frame}
%\begin{forest}
%	for tree={grow'=south}
%	[Outlook
%	[Humidity, edge label={node[midway,fill=gray,font=\scriptsize]{Sunny}} []]
%	[Wind, edge label={node[midway,fill=white,font=\scriptsize]{Rain}} [] [] []]
%	[Yes, edge label={node[midway,fill=white,font=\scriptsize]{Overcast}}]
%	]
%\end{forest}
%\end{frame}

\begin{frame}{Claude Shannon (1948): The Birth of Information Theory}
\begin{definitionbox}{The Big Idea}
Information is inversely related to probability. \textbf{Rare events are more informative!}
\end{definitionbox}

\pause
\textbf{Think about it:} Which headline tells you more?
\begin{itemize}
\item "The sun rose this morning" 
\item "It snowed in Death Valley in July"
\end{itemize}

\pause
The second one! Because it's \textbf{unexpected}.

\pause
\textbf{Shannon's insight:} The amount of information in an event should be inversely proportional to its probability.
\end{frame}

\begin{frame}{Measuring Surprise: Step by Step}
\textbf{Shannon's Information Formula:}
$$I(x) = -\log_2 p(x)$$

\pause
\textbf{Why the negative log?}
\begin{itemize}
\item Probabilities are between 0 and 1
\item $\log_2$ of values $< 1$ gives negative numbers
\item We want information to be positive
\item Hence the negative sign!
\end{itemize}

\pause
\textbf{Why base 2?} So information is measured in \textbf{bits}.
\end{frame}

\begin{frame}{Calculating Surprise: Detailed Examples}
\textbf{Example 1: Summer weather in Phoenix}
\begin{itemize}
\item Sunny day: $p = 0.9$
\pause
\item Information: $I = -\log_2(0.9) = -(-0.152) = 0.152$ bits
\pause
\item \textcolor{blue}{\textbf{Low surprise}} - we expect sunny weather
\end{itemize}

\pause
\textbf{Example 2: Snow in Phoenix in July}
\begin{itemize}
\item Probability: $p = 0.0001$ (extremely rare!)
\pause
\item Information: $I = -\log_2(0.0001) = -(-13.29) = 13.29$ bits
\pause
\item \textcolor{red}{\textbf{High surprise}} - this would be shocking news!
\end{itemize}

\pause
\textbf{Notice:} Rare events carry $\sim$90× more information!
\end{frame}

\begin{frame}{From Single Events to Distributions}
\textbf{Question:} What if we have multiple possible outcomes?

\pause
\textbf{Example:} Weather in Seattle (4 possibilities)
\begin{itemize}
\item Rainy: $p = 0.5$
\item Cloudy: $p = 0.3$  
\item Sunny: $p = 0.15$
\item Snowy: $p = 0.05$
\end{itemize}

\pause
\textbf{Problem:} Each day gives different amounts of information!
\begin{itemize}
\item If it's rainy: $I = -\log_2(0.5) = 1.0$ bit
\item If it's sunny: $I = -\log_2(0.15) = 2.74$ bits  
\item If it's snowy: $I = -\log_2(0.05) = 4.32$ bits
\end{itemize}

\pause
\textbf{Solution:} Take the \textbf{expected} (average) information!
\end{frame}

\begin{frame}{Entropy: Expected Information}
\begin{definitionbox}{Entropy Formula}
$$H(X) = \mathbb{E}[I(X)] = -\sum_{i} p(x_i) \log_2 p(x_i)$$
\textbf{Entropy} = Expected amount of information per observation
\end{definitionbox}

\pause
\textbf{Seattle weather calculation:}
\begin{align}
H &= -p(\text{rain}) \log_2 p(\text{rain}) - p(\text{cloudy}) \log_2 p(\text{cloudy}) \\
&\quad - p(\text{sunny}) \log_2 p(\text{sunny}) - p(\text{snow}) \log_2 p(\text{snow})\\
\pause
&= -0.5 \log_2(0.5) - 0.3 \log_2(0.3) - 0.15 \log_2(0.15) - 0.05 \log_2(0.05)\\
\pause  
&= 0.5(1.0) + 0.3(1.74) + 0.15(2.74) + 0.05(4.32)\\
\pause
&= 0.5 + 0.52 + 0.41 + 0.22 = \mathbf{1.65} \text{ bits}
\end{align}
\end{frame}

\begin{frame}{Entropy Intuition: Extreme Cases}
\textbf{Case 1: Completely predictable}
\begin{itemize}
\item Desert: Always sunny ($p = 1.0$)
\pause
\item $H = -1.0 \log_2(1.0) = -1.0 \times 0 = \mathbf{0}$ bits
\pause
\item \textcolor{blue}{\textbf{Zero entropy}} = No surprise = Completely predictable
\end{itemize}

\pause
\textbf{Case 2: Maximum uncertainty}  
\begin{itemize}
\item Fair coin: Heads/Tails equally likely ($p = 0.5$ each)
\pause
\item $H = -0.5 \log_2(0.5) - 0.5 \log_2(0.5) = 0.5(1) + 0.5(1) = \mathbf{1.0}$ bit
\pause
\item \textcolor{red}{\textbf{Maximum entropy}} = Maximum surprise = Completely unpredictable
\end{itemize}

\pause
\textbf{Key insight:} Entropy ranges from 0 (certain) to $\log_2(n)$ (uniform over $n$ outcomes)
\end{frame}

\begin{frame}{Entropy in Decision Trees: The Connection}
\textbf{Why do we care about entropy in ML?}

\pause
\begin{examplebox}{Decision Tree Goal}
We want to split data into \textbf{pure} subsets where we can make confident predictions.
\end{examplebox}

\pause
\begin{itemize}
\item \textbf{Pure node}: All examples same class → \textcolor{blue}{Low entropy} → \textcolor{blue}{Good split}
\pause
\item \textbf{Mixed node}: Examples from different classes → \textcolor{red}{High entropy} → \textcolor{red}{Bad split}
\end{itemize}

\pause
\textbf{Strategy:} Choose splits that \textbf{reduce entropy} the most!

\pause
This is exactly what \textbf{Information Gain} measures.
\end{frame}

\begin{frame}{Entropy}
 Statistical measure to characterize the
(im)purity of examples

\pause $H(X) = -\sum_{i=1}^k p(x_i) \log_2 p(x_i)$

\begin{figure}[htp]
    \centering
    \begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/entropy.html}
      \includegraphics[scale=0.6]{../assets/decision-trees/figures/entropy.pdf}
    \end{notebookbox}
  \end{figure}

\end{frame}
	
\begin{frame}{Towards biggest estimated performance gain}
\begin{columns}
	\begin{column}{.7\textwidth}
		
		
		\begin{scriptsize}
			

			\begin{tabular}{lllll||l} \toprule
				\textbf{Day} & \textbf{Outlook}  & \textbf{Temp} & \textbf{Humidity} & \textbf{Windy}  & \textbf{Play} \\ \midrule
				D1  & Sunny    & Hot  & High     & Weak   & No   \\
				D2  & Sunny    & Hot  & High     & Strong & No   \\
				D3  & Overcast & Hot  & High     & Weak   & Yes  \\
				D4  & Rain     & Mild & High     & Weak   & Yes  \\
				D5  & Rain     & Cool & Normal   & Weak   & Yes  \\
				D6  & Rain     & Cool & Normal   & Strong & No   \\
				D7  & Overcast & Cool & Normal   & Strong & Yes  \\
				D8  & Sunny    & Mild & High     & Weak   & No   \\
				D9  & Sunny    & Cool & Normal   & Weak   & Yes  \\
				D10 & Rain     & Mild & Normal   & Weak   & Yes  \\
				D11 & Sunny    & Mild & Normal   & Strong & Yes  \\
				D12 & Overcast & Mild & High     & Strong & Yes  \\
				D13 & Overcast & Hot  & Normal   & Weak   & Yes  \\
				D14 & Rain     & Mild & High     & Strong & No  \\ \bottomrule
			\end{tabular}
		\end{scriptsize}
	\end{column}
	\begin{column}{0.4\textwidth}
		\begin{scriptsize}
			\begin{itemize}
				\pause \item Can we use Outlook as the root node?
				\pause 	\item When Outlook is overcast, we always Play and thus no ``disagreement'' 
			\end{itemize}
			
		\end{scriptsize}
	\end{column}
	
\end{columns}

\end{frame}
	
\begin{frame}{Information Gain}
 Reduction in entropy
by partitioning examples (S) on attribute A
$$
\operatorname{Gain}(S, A) \equiv \text{Entropy}(S)-\sum_{v \in \text{Values}(A)} \frac{|S_{v}|}{|S|} \text{Entropy}(S_{v})
$$
\end{frame}

\begin{frame}{Pop Quiz \stepcounter{popquiz}\#\thepopquiz}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Quick Question!]
What does entropy measure in the context of decision trees?
\begin{enumerate}[A)]
\item The depth of the tree
\item The impurity or "disagreement" in a set of examples
\item The number of features in the dataset
\item The accuracy of the tree
\end{enumerate}
\pause
\textbf{Answer: B) The impurity or ``disagreement'' in a set of examples} - Higher entropy means more mixed classes, lower entropy means more pure subsets.
\end{tcolorbox}
\end{frame}


\begin{frame}{ID3 (Examples, Target Attribute, Attributes)}
\begin{itemize}[<+->]
	\item Create a root node for tree
	\item If all examples are +/-, return root with label = +/-
	\item  If attributes = empty, return root with most common value of
	Target Attribute in Examples
	\item Begin
	\begin{itemize}[<+->]
		\item A $\leftarrow$ attribute from Attributes which best classifies
		Examples
		\item 	Root $\leftarrow$ A
		\item  For each value (v) of A
		\begin{itemize}[<+->]
			\item Add new tree branch : A = v
			\item  Examples\textsubscript{v}: subset of examples that A = v
			\item If Examples\textsubscript{v}is empty: add leaf with label = most
			common value of Target Attribute
			\item Else: ID3 (Examples\textsubscript{v}, Target attribute, Attributes - {A})
		\end{itemize}
	\end{itemize}


\end{itemize}
\end{frame}

\begin{frame}{Learnt Decision Tree}
\begin{tikzpicture}[
node/.style={%
	draw,
	rectangle,
},
]

\node [node] (A) {Root Node (empty)};

\end{tikzpicture}

\end{frame}

	\begin{frame}{Training Data}
\begin{tabular}{lllll||l} \toprule
	\textbf{Day} & \textbf{Outlook}  & \textbf{Temp} & \textbf{Humidity} & \textbf{Windy}  & \textbf{Play} \\ \midrule
	D1  & Sunny    & Hot  & High     & Weak   & No   \\
	D2  & Sunny    & Hot  & High     & Strong & No   \\
	D3  & Overcast & Hot  & High     & Weak   & Yes  \\
	D4  & Rain     & Mild & High     & Weak   & Yes  \\
	D5  & Rain     & Cool & Normal   & Weak   & Yes  \\
	D6  & Rain     & Cool & Normal   & Strong & No   \\
	D7  & Overcast & Cool & Normal   & Strong & Yes  \\
	D8  & Sunny    & Mild & High     & Weak   & No   \\
	D9  & Sunny    & Cool & Normal   & Weak   & Yes  \\
	D10 & Rain     & Mild & Normal   & Weak   & Yes  \\
	D11 & Sunny    & Mild & Normal   & Strong & Yes  \\
	D12 & Overcast & Mild & High     & Strong & Yes  \\
	D13 & Overcast & Hot  & Normal   & Weak   & Yes  \\
	D14 & Rain     & Mild & High     & Strong & No  \\ \bottomrule
\end{tabular}
\end{frame}


\begin{frame}{Entropy calculated}
We have 14 examples in $S$: 5 No, 9 Yes

$$\Entropy(S) = -p_{\text{No}} \log_2 p_{\text{No}} - p_{\text{Yes}} \log_2 p_{\text{Yes}}$$
$$
= -\frac{5}{14} \log_2\left(\frac{5}{14}\right) - \frac{9}{14} \log_2\left(\frac{9}{14}\right) = 0.940
$$
\end{frame}

\begin{frame}{Information Gain for Outlook}
\begin{tabular}{l|l} \toprule
\textbf{Outlook} & \textbf{Play} \\ \midrule
Sunny    & No   \\
Sunny    & No   \\
Overcast & Yes  \\
Rain     & Yes  \\
Rain     & Yes  \\
Rain     & No   \\
Overcast & Yes  \\
Sunny    & No   \\
Sunny    & Yes  \\
Rain     & Yes  \\
Sunny    & Yes  \\
Overcast & Yes  \\
Overcast & Yes  \\
Rain     & No  \\ \bottomrule
\end{tabular}
\end{frame}

\begin{frame}{Information Gain for Outlook}
\begin{columns}


\begin{column}{.32\textwidth}
	\begin{table}
\begin{tabular}{l|l} \toprule
	\textbf{Outlook} & \textbf{Play} \\ \midrule
	Sunny    & No   \\
	Sunny    & No   \\
	Sunny    & No   \\
	Sunny    & Yes  \\
	Sunny    & Yes  \\

\bottomrule
\end{tabular}
We have 2 Yes, 3 No
$\Entropy = -\frac{3}{5} \log_2\left(\frac{3}{5}\right) - \frac{2}{5} \log_2\left(\frac{2}{5}\right) = 0.971$
\end{table}
\end{column}

\pause \begin{column}{.32\textwidth}
	\begin{table}


\begin{tabular}{l|l} \toprule
	\textbf{Outlook} & \textbf{Play} \\ \midrule

	Overcast & Yes  \\
	Overcast & Yes  \\
	Overcast & Yes  \\
	Overcast & Yes  \\ \bottomrule

\end{tabular}
We have 4 Yes, 0 No
$\Entropy = 0$ (pure subset)


	\end{table}


\end{column}


\pause \begin{column}{.32\textwidth}
	\begin{table}
\begin{tabular}{l|l} \toprule
	\textbf{Outlook} & \textbf{Play} \\ \midrule
	Rain     & Yes  \\
	Rain     & Yes  \\
	Rain     & No   \\
	Rain     & Yes  \\
	Rain     & No  \\ \bottomrule
\end{tabular}
We have 3 Yes, 2 No
$\Entropy = -\frac{3}{5} \log_2\left(\frac{3}{5}\right) - \frac{2}{5} \log_2\left(\frac{2}{5}\right) = 0.971$
\end{table}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Information Gain}
$$
\operatorname{Gain}(S, \text{Outlook}) = \text{Entropy}(S)-\sum_{v \in \{\text{Rain, Sunny, Overcast}\}} \frac{|S_{v}|}{|S|} \text{Entropy}(S_{v}) 
$$

\[
\Gain(S, \text{Outlook}) = \Entropy(S) - \frac{5}{14} \Entropy(S_{\text{Sunny}}) - \frac{4}{14} \Entropy(S_{\text{Overcast}}) - \frac{5}{14} \Entropy(S_{\text{Rain}})
\]
\[
= 0.940 - \frac{5}{14} \times 0.971 - \frac{4}{14} \times 0 - \frac{5}{14} \times 0.971 = 0.940 - 0.347 - 0 - 0.347 = 0.246
\]

\end{frame}

\begin{frame}{Information Gain}


\begin{tikzpicture}
\begin{axis}[
symbolic x coords={Outlook,Humidity,Wind,Temperature},
xtick=data,xtick pos=left, width=8cm,
ytick pos=left,axis x line*=bottom,nodes near coords,hide y axis,title=Information Gain, ymin=0.0]
\addplot[ybar,fill=black] coordinates {
	(Outlook,0.246)
	(Humidity,0.151)
	(Wind,0.048)
	(Temperature,0.029)
};
\end{axis}
\end{tikzpicture}


\end{frame}

\begin{frame}{Learnt Decision Tree}
\begin{tikzpicture}[
node/.style={%
	draw,
	rectangle,
},
]

\node [node] (A) {Outlook};
\path (A) ++(-150:\nodeDist) node [node] (B) {?};
\path (A) ++(-90:\nodeDist/2) node [node, fill=green] (C) {Yes};
\path (A) ++(-30:\nodeDist) node [node] (D) {?};


\draw (A) -- (B) node [left,pos=0.25] {Sunny}(A);
\draw (A) -- (C) node [right,pos=0.8] {Overcast}(A);
\draw (A) -- (D) node [right,pos=0.5] {Rain}(A);

\end{tikzpicture}

\end{frame}

	\begin{frame}{Calling ID3 on Outlook=Sunny}
\begin{tabular}{llll||l} \toprule
	\textbf{Day} & \textbf{Temp} & \textbf{Humidity} & \textbf{Windy}  & \textbf{Play} \\ \midrule
	D1    & Hot  & High     & Weak   & No   \\
	D2     & Hot  & High     & Strong & No   \\
	D8     & Mild & High     & Weak   & No   \\
	D9    & Cool & Normal   & Weak   & Yes  \\
	D11    & Mild & Normal   & Strong & Yes  \\ \bottomrule
\end{tabular}

\begin{itemize}
	\pause \item Gain($S_{\text{Outlook=Sunny}}$, Temp) = Entropy(2 Yes, 3 No) - (2/5)*Entropy(0 Yes, 2 No) -(2/5)*Entropy(1 Yes, 1 No) - (1/5)*Entropy(1 Yes, 0 No) 
	\pause \item Gain($S_{\text{Outlook=Sunny}}$, Humidity) = Entropy(2 Yes, 3 No) - (2/5)*Entropy(2 Yes, 0 No) -(3/5)*Entropy(0 Yes, 3 No) $\implies$ \textbf{maximum possible for the set}
	\pause \item Gain($S_{\text{Outlook=Sunny}}$, Windy) = Entropy(2 Yes, 3 No) - (3/5)*Entropy(1 Yes, 2 No) -(2/5)*Entropy(1 Yes, 1 No) 
\end{itemize}
\end{frame}

\begin{frame}{Learnt Decision Tree}
\begin{tikzpicture}[
node/.style={%
	draw,
	rectangle,
},
]

\node [node] (A) {Outlook};
\path (A) ++(-150:\nodeDist) node [node] (B) {Humidity};
\path (A) ++(-90:\nodeDist/2) node [node, fill=green] (C) {Yes};
\path (A) ++(-30:\nodeDist) node [node] (D) {?};
\path (B) ++(-135:\nodeDist) node [node, fill=red] (E) {No};
\path (B) ++(-45:\nodeDist) node [node, fill=green] (F) {Yes};
;

\draw (A) -- (B) node [left,pos=0.25] {Sunny}(A);
\draw (A) -- (C) node [right,pos=0.8] {Overcast}(A);
\draw (A) -- (D) node [right,pos=0.5] {Rain}(A);
\draw (B) -- (E) node [left,pos=0.25] {High}(A);
\draw (B) -- (F) node [right,pos=0.25] {Normal}(C);

\end{tikzpicture}

\end{frame}


	\begin{frame}{Calling ID3 on (Outlook=Rain)}
\begin{tabular}{llll||l} \toprule
	\textbf{Day}  & \textbf{Temp} & \textbf{Humidity} & \textbf{Windy}  & \textbf{Play} \\ \midrule
	D4    & Mild & High     & Weak   & Yes  \\
	D5     & Cool & Normal   & Weak   & Yes  \\
	D6      & Cool & Normal   & Strong & No   \\
	D10     & Mild & Normal   & Weak   & Yes  \\
	D14      & Mild & High     & Strong & No  \\ \bottomrule
\end{tabular}
\begin{itemize}
	\item The attribute Windy gives the highest information gain
\end{itemize}
\end{frame}

\begin{frame}{Learnt Decision Tree}
\begin{tikzpicture}[
node/.style={%
	draw,
	rectangle,
},
]

\node [node] (A) {Outlook};
\path (A) ++(-150:\nodeDist) node [node] (B) {Humidity};
\path (A) ++(-90:\nodeDist/2) node [node, fill=green] (C) {Yes};
\path (A) ++(-30:\nodeDist) node [node] (D) {Wind};
\path (B) ++(-135:\nodeDist) node [node, fill=red] (E) {No};
\path (B) ++(-45:\nodeDist) node [node, fill=green] (F) {Yes};
\path (D) ++(-45:\nodeDist) node [node, fill=red] (G) {No};
\path (D) ++(-135:\nodeDist) node [node, fill=green] (H) {Yes};

\draw (A) -- (B) node [left,pos=0.25] {Sunny}(A);
\draw (A) -- (C) node [right,pos=0.8] {Overcast}(A);
\draw (A) -- (D) node [right,pos=0.5] {Rain}(A);
\draw (B) -- (E) node [left,pos=0.25] {High}(A);
\draw (B) -- (F) node [right,pos=0.25] {Normal}(C);
\draw (D) -- (G) node [right,pos=0.25] {Strong}(A);
\draw (D) -- (H) node [left,pos=0.25] {Weak}(A);
\end{tikzpicture}

\end{frame}

\begin{frame}{Prediction for Decision Tree}
Just walk down the tree!

\begin{tikzpicture}[
node/.style={%
	draw,
	rectangle,
},
]

\node [node] (A) {Outlook};
\path (A) ++(-150:\nodeDist) node [node] (B) {Humidity};
\path (A) ++(-90:\nodeDist/2) node [node, fill=green] (C) {Yes};
\path (A) ++(-30:\nodeDist) node [node] (D) {Wind};
\path (B) ++(-135:\nodeDist) node [node, fill=red] (E) {No};
\path (B) ++(-45:\nodeDist) node [node, fill=green] (F) {Yes};
\path (D) ++(-45:\nodeDist) node [node, fill=red] (G) {No};
\path (D) ++(-135:\nodeDist) node [node, fill=green] (H) {Yes};

\draw (A) -- (B) node [left,pos=0.25] {Sunny}(A);
\draw (A) -- (C) node [right,pos=0.8] {Overcast}(A);
\draw (A) -- (D) node [right,pos=0.5] {Rain}(A);
\draw (B) -- (E) node [left,pos=0.25] {High}(A);
\draw (B) -- (F) node [right,pos=0.25] {Normal}(C);
\draw (D) -- (G) node [right,pos=0.25] {Strong}(A);
\draw (D) -- (H) node [left,pos=0.25] {Weak}(A);
\end{tikzpicture}

\pause Prediction for $<$High Humidity, Strong Wind, Sunny Outlook, Hot Temp$>$ is ? \\
\pause  No
\end{frame}

\begin{frame}{Limiting Tree Depth}
\begin{definitionbox}{Depth-Limited Trees}
When depth limit is reached, assign the \textbf{most common class} in that path as the leaf node prediction.
\end{definitionbox}

\pause
\begin{itemize}
\item \textbf{Depth-0 tree} (no decisions):
\begin{itemize}
\item Always predict the most common class
\item For our dataset: Always predict \textcolor{green}{\textbf{Yes}}
\end{itemize}

\pause
\item \textbf{Depth-1 tree} (single decision):
\end{itemize}

\begin{center}
\begin{tikzpicture}[
node/.style={%
	draw,
	rectangle,
},
]

\node [node] (A) {Outlook};
\path (A) ++(-150:\nodeDist) node [node, fill=red] (B) {No};
\path (A) ++(-90:\nodeDist/2) node [node, fill=green] (C) {Yes};
\path (A) ++(-30:\nodeDist) node [node, fill=green] (D) {Yes};

\draw (A) -- (B) node [left,pos=0.25] {Sunny}(A);
\draw (A) -- (C) node [right,pos=0.8] {Overcast}(A);
\draw (A) -- (D) node [right,pos=0.5] {Rain}(A);

\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}{Pop Quiz \stepcounter{popquiz}\#\thepopquiz}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Quick Question!]
In the tennis dataset, why did "Outlook" have the highest information gain?
\begin{enumerate}[A)]
\item It was the first feature in the dataset
\item When Outlook=Overcast, all examples have Play=Yes (pure subset)
\item It has the most possible values
\item It was chosen randomly
\end{enumerate}
\pause
\textbf{Answer: B) When Outlook=Overcast, all examples have Play=Yes} - This creates a pure subset with entropy=0, maximizing information gain.
\end{tcolorbox}
\end{frame}

\section{Discrete Input, Real Output}

\begin{frame}{Modified Dataset}
\begin{table}[]
	\begin{tabular}{@{}llllll@{}}
		\toprule
		\textbf{Day} & \textbf{Outlook} & \textbf{Temp} & \textbf{Humidity} & \textbf{Wind} & \textbf{Minutes Played} \\ \midrule
		D1           & Sunny            & Hot           & High              & Weak          & 20                      \\
		D2           & Sunny            & Hot           & High              & Strong        & 24                      \\
		D3           & Overcast         & Hot           & High              & Weak          & 40                      \\
		D4           & Rain             & Mild          & High              & Weak          & 50                      \\
		D5           & Rain             & Cool          & Normal            & Weak          & 60                      \\
		D6           & Rain             & Cool          & Normal            & Strong        & 10                      \\
		D7           & Overcast         & Cool          & Normal            & Strong        & 4                       \\
		D8           & Sunny            & Mild          & High              & Weak          & 10                      \\
		D9           & Sunny            & Cool          & Normal            & Weak          & 60                      \\
		D10          & Rain             & Mild          & Normal            & Weak          & 40                      \\
		D11          & Sunny            & Mild          & High              & Strong        & 45                      \\
		D12          & Overcast         & Mild          & High              & Strong        & 40                      \\
		D13          & Overcast         & Hot           & Normal            & Weak          & 35                      \\
		D14          & Rain             & Mild          & High              & Strong        & 20                      \\ \bottomrule
	\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Regression Trees: From Classification to Regression}
\begin{itemize}
	\item \pause Classification trees predict discrete classes (Yes/No, categories)
	\item \pause Regression trees predict continuous numeric values  
	\item \pause \textbf{Key Question:} How do we measure impurity for continuous outputs?
	\item \pause For classification: Used entropy, information gain
	\item \pause For regression: Use \textbf{Mean Squared Error (MSE)}
\end{itemize}

\pause
\begin{keypointsbox}
\textbf{Why MSE for Regression?}\\
MSE measures how far predicted values are from actual values.\\
Lower MSE = Better predictions = Less "impurity" in the data
\end{keypointsbox}
\end{frame}

\begin{frame}{Mean Squared Error (MSE): The Mathematics}
\begin{definitionbox}{Mean Squared Error}
For a dataset $S$ with $n$ data points and target values $y_1, y_2, \ldots, y_n$:

\vspace{0.3cm}
\[
\MSE(S) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2
\]
\vspace{0.3cm}

where $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ is the mean of target values
\end{definitionbox}

\pause
\begin{itemize}
	\item \pause $(y_i - \bar{y})^2$: Squared difference between actual and mean
	\item \pause Squaring ensures positive values and penalizes large errors
	\item \pause MSE = 0 when all values are identical (perfect homogeneity)
	\item \pause Higher MSE = More variation = Higher impurity
\end{itemize}
\end{frame}

\begin{frame}{MSE Calculation: Step 1 - The Complete Dataset}
\begin{center}
\begin{table}[]
	\begin{tabular}{@{}ll@{}}
		\toprule
		\textbf{Wind} & \textbf{Minutes Played} \\ \midrule
		Weak          & 20                      \\
		Strong        & 24                      \\
		Weak          & 40                      \\
		Weak          & 50                      \\
		Weak          & 60                      \\
		Strong        & 10                      \\
		Strong        & 4                       \\
		Weak          & 10                      \\
		Weak          & 60                      \\
		Weak          & 40                      \\
		Strong        & 45                      \\
		Strong        & 40                      \\
		Weak          & 35                      \\
		Strong        & 20                      \\ \bottomrule
	\end{tabular}
\end{table}
\end{center}

\begin{itemize}
	\item \textbf{Tennis Dataset:} Predicting minutes played (continuous target)
	\item \textbf{Goal:} Calculate MSE for the entire dataset $S$
	\item \textbf{Step 1:} Find the mean $\bar{y}$ of all target values
\end{itemize}
\end{frame}

\begin{frame}{MSE Calculation: Step 2 - Computing the Mean}
\begin{examplebox}{Calculating Mean Minutes Played}
\textbf{All target values:} 20, 24, 40, 50, 60, 10, 4, 10, 60, 40, 45, 40, 35, 20

\vspace{0.3cm}
\textbf{Step 1:} Sum all values
\[
\sum y_i = 20 + 24 + 40 + 50 + 60 + 10 + 4 + 10 + 60 + 40 + 45 + 40 + 35 + 20
\]
\pause
\[
= 458
\]

\pause
\textbf{Step 2:} Divide by number of data points ($n = 14$)
\[
\bar{y} = \frac{458}{14} = 32.71 \text{ minutes}
\]
\end{examplebox}
\end{frame}

\begin{frame}{MSE Calculation: Step 3 - Computing Squared Differences}
\begin{examplebox}{Calculating $(y_i - \bar{y})^2$ for Each Data Point}
\scriptsize
With $\bar{y} = 32.71$:

\vspace{0.2cm}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{$y_i$} & \textbf{$y_i - \bar{y}$} & \textbf{$(y_i - \bar{y})^2$} \\ \hline
20 & $20 - 32.71 = -12.71$ & $(-12.71)^2 = 161.54$ \\
24 & $24 - 32.71 = -8.71$ & $(-8.71)^2 = 75.86$ \\
40 & $40 - 32.71 = 7.29$ & $(7.29)^2 = 53.14$ \\
50 & $50 - 32.71 = 17.29$ & $(17.29)^2 = 299.14$ \\
60 & $60 - 32.71 = 27.29$ & $(27.29)^2 = 744.74$ \\
10 & $10 - 32.71 = -22.71$ & $(-22.71)^2 = 515.74$ \\
4 & $4 - 32.71 = -28.71$ & $(-28.71)^2 = 824.26$ \\
\hline
\end{tabular}
\end{examplebox}
\pause
\textbf{Continue this for all 14 data points...}
\end{frame}

\begin{frame}{MSE Calculation: Step 4 - Complete Squared Differences}
\begin{examplebox}{All Squared Differences}
\tiny
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{$y_i$} & \textbf{$y_i - \bar{y}$} & \textbf{$(y_i - \bar{y})^2$} \\ \hline
20 & $-12.71$ & $161.54$ \\
24 & $-8.71$ & $75.86$ \\
40 & $7.29$ & $53.14$ \\
50 & $17.29$ & $299.14$ \\
60 & $27.29$ & $744.74$ \\
10 & $-22.71$ & $515.74$ \\
4 & $-28.71$ & $824.26$ \\
10 & $-22.71$ & $515.74$ \\
60 & $27.29$ & $744.74$ \\
40 & $7.29$ & $53.14$ \\
45 & $12.29$ & $151.04$ \\
40 & $7.29$ & $53.14$ \\
35 & $2.29$ & $5.24$ \\
20 & $-12.71$ & $161.54$ \\ \hline
\textbf{Sum} & & $\mathbf{4358.86}$ \\
\hline
\end{tabular}
\end{examplebox}
\end{frame}

\begin{frame}{MSE Calculation: Step 5 - Final MSE Computation}
\begin{examplebox}{Computing MSE for Complete Dataset}
\textbf{Formula:} 
\[
\MSE(S) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2
\]

\pause
\textbf{Substituting our values:}
\[
\MSE(S) = \frac{1}{14} \times 4358.86 = 311.35
\]

\pause
\textbf{Interpretation:}
\begin{itemize}
	\item MSE = 311.35 square-minutes
	\item This measures the "impurity" or variation in our dataset
	\item Higher MSE = More variation in target values
	\item When we split the data, we want to reduce this MSE
\end{itemize}
\end{examplebox}
\end{frame}

\begin{frame}{MSE Reduction: The Splitting Criterion}
\begin{definitionbox}{MSE Reduction Formula}
For a split on attribute $A$ with values $v_1, v_2, \ldots, v_k$:

\vspace{0.3cm}
\[
\text{MSE Reduction} = \MSE(S) - \sum_{j=1}^{k} \frac{|S_{v_j}|}{|S|} \times \MSE(S_{v_j})
\]
\vspace{0.3cm}

where:
\begin{itemize}
	\item $S$ is the original dataset
	\item $S_{v_j}$ is the subset with attribute value $v_j$
	\item $|S_{v_j}|$ is the size of subset $S_{v_j}$
	\item $|S|$ is the size of original dataset
\end{itemize}
\end{definitionbox}

\pause
\begin{keypointsbox}
\textbf{Key Insight:} MSE Reduction $> 0$ means the split improves our model!\\
\textbf{Choose the split with highest MSE Reduction}
\end{keypointsbox}
\end{frame}

\begin{frame}{Splitting on Wind: Step 1 - Partition the Data}
\begin{columns}
\begin{column}{.5\textwidth}
\begin{examplebox}{Wind = Weak (8 points)}
\scriptsize
\begin{tabular}{|c|c|}
\hline
\textbf{Wind} & \textbf{Minutes} \\ \hline
Weak & 20 \\
Weak & 40 \\
Weak & 50 \\
Weak & 60 \\
Weak & 10 \\
Weak & 60 \\
Weak & 40 \\
Weak & 35 \\ \hline
\end{tabular}
\end{examplebox}
\end{column}

\begin{column}{.5\textwidth}
\begin{examplebox}{Wind = Strong (6 points)}
\scriptsize
\begin{tabular}{|c|c|}
\hline
\textbf{Wind} & \textbf{Minutes} \\ \hline
Strong & 24 \\
Strong & 10 \\
Strong & 4 \\
Strong & 45 \\
Strong & 40 \\
Strong & 20 \\ \hline
\end{tabular}
\end{examplebox}
\end{column}
\end{columns}

\vspace{0.3cm}
\begin{itemize}
	\item \textbf{Original dataset:} 14 points, MSE = 311.35
	\item \textbf{After split:} 8 points (Weak) + 6 points (Strong)
	\item \textbf{Next:} Calculate MSE for each subset
\end{itemize}
\end{frame}

\begin{frame}{Splitting on Wind: Step 2 - MSE for Wind=Weak}
\begin{examplebox}{Calculating $\MSE(S_{\text{Wind=Weak}})$}
\textbf{Data points:} 20, 40, 50, 60, 10, 60, 40, 35

\pause
\textbf{Step 1:} Calculate mean
\[
\bar{y}_{\text{weak}} = \frac{20 + 40 + 50 + 60 + 10 + 60 + 40 + 35}{8} = \frac{315}{8} = 39.375
\]

\pause
\textbf{Step 2:} Calculate squared differences
\scriptsize
\begin{tabular}{|c|c|c|}
\hline
$y_i$ & $y_i - 39.375$ & $(y_i - 39.375)^2$ \\ \hline
20 & $-19.375$ & $375.39$ \\
40 & $0.625$ & $0.39$ \\
50 & $10.625$ & $112.89$ \\
60 & $20.625$ & $425.39$ \\
10 & $-29.375$ & $862.89$ \\
60 & $20.625$ & $425.39$ \\
40 & $0.625$ & $0.39$ \\
35 & $-4.375$ & $19.14$ \\ \hline
\textbf{Sum} & & $\mathbf{2221.87}$ \\
\hline
\end{tabular}
\end{examplebox}
\end{frame}

\begin{frame}{Splitting on Wind: Step 3 - Complete MSE for Wind=Weak}
\begin{examplebox}{Final MSE Calculation for Wind=Weak}
\[
\MSE(S_{\text{Wind=Weak}}) = \frac{1}{8} \times 2221.87 = 277.73
\]
\end{examplebox}

\pause
\begin{examplebox}{Verification Check}
\begin{itemize}
	\item Original MSE for all data: 311.35
	\item MSE for Wind=Weak subset: 277.73
	\item \textbf{Good sign:} MSE decreased (less variation within this group)
	\item This subset is more "homogeneous" than the full dataset
\end{itemize}
\end{examplebox}
\end{frame}

\begin{frame}{Splitting on Wind: Step 4 - MSE for Wind=Strong}
\begin{examplebox}{Calculating $\MSE(S_{\text{Wind=Strong}})$}
\textbf{Data points:} 24, 10, 4, 45, 40, 20

\pause
\textbf{Step 1:} Calculate mean
\[
\bar{y}_{\text{strong}} = \frac{24 + 10 + 4 + 45 + 40 + 20}{6} = \frac{143}{6} = 23.83
\]

\pause
\textbf{Step 2:} Calculate squared differences
\scriptsize
\begin{tabular}{|c|c|c|}
\hline
$y_i$ & $y_i - 23.83$ & $(y_i - 23.83)^2$ \\ \hline
24 & $0.17$ & $0.03$ \\
10 & $-13.83$ & $191.27$ \\
4 & $-19.83$ & $393.23$ \\
45 & $21.17$ & $448.17$ \\
40 & $16.17$ & $261.47$ \\
20 & $-3.83$ & $14.67$ \\ \hline
\textbf{Sum} & & $\mathbf{1308.84}$ \\
\hline
\end{tabular}

\pause
\[
\MSE(S_{\text{Wind=Strong}}) = \frac{1}{6} \times 1308.84 = 218.14
\]
\end{examplebox}
\end{frame}

\begin{frame}{Splitting on Wind: Step 5 - Computing MSE Reduction}
\begin{examplebox}{Final MSE Reduction Calculation}
\textbf{We have:}
\begin{itemize}
	\item $\MSE(S) = 311.35$ (original dataset)
	\item $\MSE(S_{\text{Wind=Weak}}) = 277.73$ (8 points)
	\item $\MSE(S_{\text{Wind=Strong}}) = 218.14$ (6 points)
\end{itemize}

\pause
\textbf{Weighted Average MSE:}
\[
\text{Weighted MSE} = \frac{8}{14} \times 277.73 + \frac{6}{14} \times 218.14
\]
\pause
\[
= 0.571 \times 277.73 + 0.429 \times 218.14
\]
\pause
\[
= 158.60 + 93.58 = 252.18
\]

\pause
\textbf{MSE Reduction:}
\[
\text{MSE Reduction} = 311.35 - 252.18 = \mathbf{59.17}
\]
\end{examplebox}
\end{frame}

\begin{frame}{MSE Reduction: Interpretation and Decision Making}
\begin{keypointsbox}
\textbf{What Does MSE Reduction = 59.17 Mean?}
\begin{itemize}
	\item \textbf{Positive value:} The split improves our model!
	\item \textbf{Magnitude:} We reduced prediction error by 59.17 square-minutes
	\item \textbf{Percentage:} $(59.17 / 311.35) \times 100\% = 19\%$ improvement
	\item \textbf{Intuition:} Wind attribute helps separate high/low playing minutes
\end{itemize}
\end{keypointsbox}

\pause
\begin{examplebox}{Decision Tree Building Process}
\begin{itemize}
	\item \textbf{Step 1:} Calculate MSE reduction for all possible splits
	\item \textbf{Step 2:} Choose the split with \emph{highest} MSE reduction
	\item \textbf{Step 3:} Recursively apply to child nodes
	\item \textbf{Stop when:} MSE reduction becomes too small or max depth reached
\end{itemize}
\end{examplebox}

\pause
\begin{alertbox}{Key Difference from Classification}
\textbf{Classification:} Use Information Gain (maximize information)\\
\textbf{Regression:} Use MSE Reduction (minimize prediction error)
\end{alertbox}
\end{frame}

\begin{frame}{Pop Quiz \stepcounter{popquiz}\#\thepopquiz}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Quick Question!]
For regression trees, what criterion do we use instead of Information Gain?
\begin{enumerate}[A)]
\item Information Gain
\item Gini Impurity
\item Mean Squared Error (MSE) Reduction
\item Accuracy
\end{enumerate}
\pause
\textbf{Answer: C) Mean Squared Error (MSE) Reduction} - For regression, we minimize MSE instead of maximizing information gain.
\end{tcolorbox}
\end{frame}

\begin{frame}{MSE Reduction for Regression Trees}

	\begin{figure}[htp]
		\centering
		\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-output.html}
		  \includegraphics[width=\linewidth]{../assets/decision-trees/figures/discrete-input-real-output-level-1.pdf}
		\end{notebookbox}
	  \end{figure}

\end{frame}

\begin{frame}{Learnt Tree}
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{../assets/decision-trees/diagrams/tree}

	\label{fig:tree}
\end{figure}

\end{frame}

\begin{frame}{Learnt Tree}
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{../assets/decision-trees/diagrams/tree2}

	\label{fig:tree}
\end{figure}

\end{frame}

\section{Real Input Discrete Output}


\begin{frame}{Finding splits}
\begin{table}[]
	\begin{tabular}{@{}lrr@{}}
		\toprule
		\textbf{Day} & \textbf{Temperature} & \textbf{PlayTennis} \\ \midrule
		D1           & 40                   & No                  \\
		D2           & 48                   & No                  \\
		D3           & 60                   & Yes                 \\
		D4           & 72                   & Yes                 \\
		D5           & 80                   & Yes                 \\
		D6           & 90                   & No                  \\ \bottomrule
	\end{tabular}
\end{table}
\begin{itemize}[<+->]
	\item How do you find splits?
	\item Sort by attribute
	\item Find potential split points (midpoints). 
	\item For the above example, we have 5 potential splits: 44, 54, 66, 76, 85
	\item Calculate the weighted impurity for each split
	\item Choose the split with the lowest impurity
\end{itemize}
\end{frame}

\begin{frame}{Finding splits}
	\begin{table}[]
		\begin{tabular}{@{}lrr@{}}
			\toprule
			\textbf{Day} & \textbf{Temperature} & \textbf{PlayTennis} \\ \midrule
			D1           & 40                   & No                  \\
			D2           & 48                   & No                  \\
			D3           & 60                   & Yes                 \\
			D4           & 72                   & Yes                 \\
			D5           & 80                   & Yes                 \\
			D6           & 90                   & No                  \\ \bottomrule
		\end{tabular}
	\end{table}
	\begin{itemize}
		\item Consider split at 44
		\item LHS has 1 No and 0 Yes; RHS has 3 Yes and 2 No
		\item Entropy for LHS = 0, Entropy for RHS = 0.971
		\item Weighted Entropy = 0.971*5/6 = 0.808
	\end{itemize}
	\end{frame}

	\begin{frame}{Finding splits}
		\begin{table}[]
			\begin{tabular}{@{}lrr@{}}
				\toprule
				\textbf{Day} & \textbf{Temperature} & \textbf{PlayTennis} \\ \midrule
				D1           & 40                   & No                  \\
				D2           & 48                   & No                  \\
				D3           & 60                   & Yes                 \\
				D4           & 72                   & Yes                 \\
				D5           & 80                   & Yes                 \\
				D6           & 90                   & No                  \\ \bottomrule
			\end{tabular}
		\end{table}
		\begin{itemize}
			\item Consider split at 54
			\item LHS has 2 No and 0 Yes; RHS has 3 Yes and 1 No
			\item Entropy for LHS = 0, Entropy for RHS = 0.811
			\item Weighted Entropy = 0.811*4/6 = 0.541
		\end{itemize}
		\end{frame}

		\begin{frame}{Finding splits}
			\begin{table}[]
				\begin{tabular}{@{}lrr@{}}
					\toprule
					\textbf{Day} & \textbf{Temperature} & \textbf{PlayTennis} \\ \midrule
					D1           & 40                   & No                  \\
					D2           & 48                   & No                  \\
					D3           & 60                   & Yes                 \\
					D4           & 72                   & Yes                 \\
					D5           & 80                   & Yes                 \\
					D6           & 90                   & No                  \\ \bottomrule
				\end{tabular}
			\end{table}
			\begin{itemize}
				\item Consider split at 66
				\item LHS has 2 No and 1 Yes; RHS has 2 Yes and 1 No
				\item Entropy for LHS = 0.918, Entropy for RHS = 0.918
				\item Weighted Entropy = 0.918*3/6 + 0.918*3/6 = 0.918
			\end{itemize}
			\end{frame}


		\begin{frame}{Finding splits}
			\begin{table}[]
				\begin{tabular}{@{}lrr@{}}
					\toprule
					\textbf{Day} & \textbf{Temperature} & \textbf{PlayTennis} \\ \midrule
					D1           & 40                   & No                  \\
					D2           & 48                   & No                  \\
					D3           & 60                   & Yes                 \\
					D4           & 72                   & Yes                 \\
					D5           & 80                   & Yes                 \\
					D6           & 90                   & No                  \\ \bottomrule
				\end{tabular}
			\end{table}
			\begin{itemize}
				\item Consider split at 76
				\item LHS has 2 No and 2 Yes; RHS has 1 Yes and 1 No
				\item Entropy for LHS = 1, Entropy for RHS = 1
				\item Weighted Entropy = 1*4/6 + 1*2/6 = 1
			\end{itemize}
			\end{frame}

			\begin{frame}{Finding splits}
				\begin{table}[]
					\begin{tabular}{@{}lrr@{}}
						\toprule
						\textbf{Day} & \textbf{Temperature} & \textbf{PlayTennis} \\ \midrule
						D1           & 40                   & No                  \\
						D2           & 48                   & No                  \\
						D3           & 60                   & Yes                 \\
						D4           & 72                   & Yes                 \\
						D5           & 80                   & Yes                 \\
						D6           & 90                   & No                  \\ \bottomrule
					\end{tabular}
				\end{table}
				\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-discrete-output.html}
					\includegraphics[scale=0.3]{../assets/decision-trees/figures/real-ip-1.pdf}
				\end{notebookbox}
				\end{frame}

				\begin{frame}{Finding splits}
					\begin{table}[]
						\begin{tabular}{@{}lrr@{}}
							\toprule
							\textbf{Day} & \textbf{Temperature} & \textbf{PlayTennis} \\ \midrule
							D1           & 40                   & No                  \\
							D2           & 48                   & No                  \\
							D3           & 60                   & Yes                 \\
							D4           & 72                   & Yes                 \\
							D5           & 80                   & Yes                 \\
							D6           & 90                   & No                  \\ \bottomrule
						\end{tabular}
					\end{table}
					\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-discrete-output.html}
						\includegraphics[scale=0.25]{../assets/decision-trees/figures/real-ip-2.pdf}
					\end{notebookbox}
					\end{frame}

\foreach \i in {1,...,9} {
\begin{frame}{Example (DT of depth \i)}
    \begin{figure}
		\centering
		\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-discrete-output.html}
			\includegraphics{../assets/decision-trees/figures/dt-\i.pdf}
		  \end{notebookbox}
    
    \end{figure}
\end{frame}
}






\begin{frame}{Pop Quiz \stepcounter{popquiz}\#\thepopquiz}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Quick Question!]
When finding splits for continuous features, how do we determine candidate split points?
\begin{enumerate}[A)]
\item Use all feature values as split points
\item Use midpoints between consecutive sorted feature values
\item Use random values within the feature range
\item Use only the minimum and maximum values
\end{enumerate}
\pause
\textbf{Answer: B) Use midpoints between consecutive sorted feature values} - This ensures we test all meaningful boundaries between different class regions.
\end{tcolorbox}
\end{frame}

\section{Real Input Real Output}

\begin{frame}{Example 1}
Let us consider the dataset given below
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics{../assets/decision-trees/figures/ri-ro-dataset.pdf}
	  \end{notebookbox}
\end{center}
\end{frame}

\begin{frame}{Example 1}
What would be the prediction for decision tree with depth 0?
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics{../assets/decision-trees/figures/ri-ro-dataset.pdf}
	  \end{notebookbox}
\end{center}
\end{frame}

\begin{frame}{Example 1}
Prediction for decision tree with depth 0.\\
Horizontal dashed line shows the predicted $Y$ value. It is the average of $Y$ values of all datapoints.\\
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics{../assets/decision-trees/figures/ri-ro-depth-0.pdf}	
	  \end{notebookbox}
\end{center}
\end{frame}


\begin{frame}{Example 1}
What would be the decision tree with depth 1?
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics{../assets/decision-trees/figures/ri-ro-dataset.pdf}
	  \end{notebookbox}
\end{center}
\end{frame}

\begin{frame}{Example 1}
Decision tree with depth 1
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics{../assets/decision-trees/figures/ri-ro-depth-1.pdf}
	\end{notebookbox}	
\end{center}
\end{frame}

\begin{frame}{Example 1}
The Decision Boundary
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics[scale=0.6]{../assets/decision-trees/figures/ri-ro-depth-1-sklearn.pdf}
	\end{notebookbox}
\end{center}
\end{frame}


\begin{frame}{Example 1}
What would be the decision tree with depth 2	?
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics{../assets/decision-trees/figures/ri-ro-dataset.pdf}
	  \end{notebookbox}
\end{center}
\end{frame}

\begin{frame}{Example 1}
	Decision tree with depth 2
	\begin{center}
		\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
			\includegraphics{../assets/decision-trees/figures/ri-ro-depth-2.pdf}
		\end{notebookbox}
	\end{center}
	\end{frame}
	
	\begin{frame}{Example 1}
	The Decision Boundary
	\begin{center}
		\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
			\includegraphics[scale=0.6]{../assets/decision-trees/figures/ri-ro-depth-2-sklearn.pdf}
		\end{notebookbox}
	\end{center}
	\end{frame}
	

\begin{frame}{Objective Function for Regression Trees}
\only<1-4>{
Feature is denoted by $X$ and target by $Y$.\\
Let the split be at $X = s$.\\
Define regions: $R_1 = \{x : x \leq s\}$ and $R_2 = \{x : x > s\}$.\\
\vspace{1cm}
}
\only<2-4>{
For each region, compute the mean prediction:\\
$c_1 = \frac{1}{|R_1|} \sum_{x_i \in R_1} y_i$ \\
$c_2 = \frac{1}{|R_2|} \sum_{x_i \in R_2} y_i$ \\
}
\only<3-4>{
The loss function is:\\
$$\text{Loss}(s) = \sum_{x_i \in R_1} (y_i - c_1)^2 + \sum_{x_i \in R_2} (y_i - c_2)^2$$
\vspace{1cm}
}
\only<4>{
Our objective is to find the optimal split:\\
$$s^* = \argmin_{s} \left[ \sum_{x_i \in R_1(s)} (y_i - c_1(s))^2 + \sum_{x_i \in R_2(s)} (y_i - c_2(s))^2 \right]$$
}
\end{frame}

\begin{frame}{Algorithm: Finding the Optimal Split}
\begin{enumerate}
\item<1-> Sort all data points $(x_i, y_i)$ in increasing order of $x_i$.
\item<2-> Evaluate the loss function for all candidate splits:
\vspace{0.25cm}
\begin{center}
$s = \frac{x_i + x_{i+1}}{2}$ for $i = 1, 2, \ldots, n-1$
\end{center}
\vspace{0.25cm}
\item<2-> Select the split $s^*$ that minimizes the loss function.
\end{enumerate} 
\end{frame}


\begin{frame}{A Question!}
Draw a regression tree for Y = sin(X), $0 \leq X \leq 2\pi$ 
\end{frame}

\begin{frame}{A Question!}
Dataset of Y = sin(X), $0 \leq X \leq 7$ with 10,000 points 
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics{../assets/decision-trees/figures/sine-dataset.pdf}
	  \end{notebookbox}
\end{center}
\end{frame}

\begin{frame}{A Question!}
Regression tree of depth 1
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics[scale=0.6]{../assets/decision-trees/figures/sine-depth-1-sklearn.pdf}
	  \end{notebookbox}
\end{center}
\end{frame}

\begin{frame}{A Question!}
Decision Boundary
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics{../assets/decision-trees/figures/sine-depth-1.pdf}
	  \end{notebookbox}
\end{center}
\end{frame}

\begin{frame}{A Question!}
Regression tree with no depth limit is too big to fit in a slide. \\
It has of depth 4. The decision boundaries are in figure below.\\
\begin{center}
	\begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/decision-tree-real-input-real-output.html}
		\includegraphics{../assets/decision-trees/figures/sine-depth-4.pdf}
	  \end{notebookbox}
\end{center}
\end{frame}

\begin{frame}{Pop Quiz \stepcounter{popquiz}\#\thepopquiz}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Quick Question!]
What is the prediction function for a regression tree leaf node?
\begin{enumerate}[A)]
\item The median of target values in that region
\item The mode of target values in that region  
\item The mean of target values in that region
\item A linear function of the features
\end{enumerate}
\pause
\textbf{Answer: C) The mean of target values in that region} - Each leaf predicts the average target value of training samples that reach that leaf.
\end{tcolorbox}
\end{frame}

\section{Pruning and Overfitting}

\begin{frame}{The Problem: Overfitting in Decision Trees}
\begin{itemize}[<+->]
\item \textbf{Unpruned trees}: Can grow very deep and complex
\item \textbf{Perfect training accuracy}: Each leaf contains single training example
\item \textbf{But}: Poor generalization to new data
\item \textbf{Symptoms}:
    \begin{itemize}
    \item High training accuracy, low test accuracy
    \item Very deep trees with many leaves
    \item Rules that are too specific to training data
    \end{itemize}
\item \textbf{Solution}: Pruning to control model complexity
\end{itemize}
\end{frame}

\begin{frame}{Pre-pruning (Early Stopping)}
\textbf{Stop growing tree before it becomes too complex}:
\begin{itemize}[<+->]
\item \textbf{Maximum depth}: Limit tree depth (e.g., max\_depth = 5)
\item \textbf{Minimum samples per split}: Don't split if node has < N samples
\item \textbf{Minimum samples per leaf}: Ensure each leaf has $\geq$ M samples
\item \textbf{Maximum features}: Consider only subset of features at each split
\item \textbf{Minimum impurity decrease}: Only split if improvement > threshold
\end{itemize}

\textbf{Advantages}: Simple, computationally efficient \\
\textbf{Disadvantages}: May stop too early, miss good splits later
\end{frame}

\begin{frame}{Post-pruning (Tree Simplification)}
\textbf{Grow full tree, then remove unnecessary branches}:
\begin{itemize}[<+->]
\item \textbf{Algorithm}:
    \begin{enumerate}
    \item Grow complete tree on training data
    \item Use validation set to evaluate subtree performance
    \item Remove branches that don't improve validation accuracy
    \item Repeat until no beneficial removals remain
    \end{enumerate}
\item \textbf{Cost Complexity Pruning}: Minimize $\text{Error} + \alpha \times \text{Tree Size}$
\item \textbf{Advantages}: More thorough, can recover from early stopping mistakes
\item \textbf{Disadvantages}: More computationally expensive
\end{itemize}
\end{frame}

\begin{frame}{Cost Complexity Pruning Algorithm}
\textbf{Systematic approach to find optimal tree size}:
\begin{itemize}[<+->]
\item \textbf{Cost function}: $R_\alpha(T) = R(T) + \alpha |T|$
    \begin{itemize}
    \item $R(T)$: Misclassification error on validation set
    \item $|T|$: Number of terminal nodes (tree size)
    \item $\alpha$: Complexity parameter (penalty for larger trees)
    \end{itemize}
\item \textbf{Process}:
    \begin{enumerate}
    \item Start with full tree ($\alpha = 0$)
    \item Gradually increase $\alpha$
    \item At each $\alpha$, prune branches that increase cost
    \item Select $\alpha$ with best cross-validation performance
    \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}{Bias-Variance Trade-off in Trees}
\begin{itemize}[<+->]
\item \textbf{Unpruned trees}:
    \begin{itemize}
    \item Low bias (can fit complex patterns)
    \item High variance (sensitive to training data changes)
    \item Prone to overfitting
    \end{itemize}
\item \textbf{Heavily pruned trees}:
    \begin{itemize}
    \item High bias (may miss important patterns)
    \item Low variance (more stable predictions)
    \item Risk of underfitting
    \end{itemize}
\item \textbf{Optimal pruning}: Balances bias and variance
\item \textbf{Cross-validation}: Essential for finding this balance
\end{itemize}
\end{frame}

\begin{frame}{Practical Pruning Guidelines}
\begin{itemize}[<+->]
\item \textbf{Start simple}: Begin with restrictive pre-pruning parameters
\item \textbf{Cross-validation}: Always use CV to select pruning parameters
\item \textbf{Validation curves}: Plot training/validation error vs. tree complexity
\item \textbf{Common parameters} (sklearn):
    \begin{itemize}
    \item \texttt{max\_depth}: Start with 3-10
    \item \texttt{min\_samples\_split}: Try 10-100
    \item \texttt{min\_samples\_leaf}: Try 5-50
    \item \texttt{ccp\_alpha}: Use for cost complexity pruning
    \end{itemize}
\item \textbf{Domain knowledge}: Consider interpretability requirements
\end{itemize}
\end{frame}

\section{Summary and Key Takeaways}

\begin{frame}{Summary}
\begin{itemize}
	\item Interpretability an important goal
\item Decision trees: well known interpretable models
\item  Learning optimal tree is hard
\item  Greedy approach:
\item  Recursively split to maximize “performance gain”
\item  Issues:
\begin{itemize}
	\item Can overfit easily!
	\item  Empirically not as powerful as other methods
\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}

	\begin{figure}
		\centering
		\includegraphics{../assets/decision-trees/figures/dt_weighted/fig1.pdf}
	\end{figure}
	
	
	\end{frame}
	
	
	\begin{frame}
	
	\begin{figure}
		\centering
		\includegraphics{../assets/decision-trees/figures/dt_weighted/fig2.pdf}
	\end{figure}
	
	
	\end{frame}
	
	
	\begin{frame}
	
	\begin{figure}
		\centering
		\includegraphics{../assets/decision-trees/figures/dt_weighted/fig2.pdf}
	\end{figure}
	
	$$\Entropy = -P(+) \log_2 P(+) - P(-) \log_2 P(-)$$
	
	$$P(+) = \frac{0.1 + 0.1 + 0.3}{1} = 0.5, \quad P(-) = \frac{0.3 + 0.1 + 0.1}{1} = 0.5$$
	
	$$\Entropy = E_s = -\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2} = 1$$
	
	\end{frame}
	
	
	\begin{section}{Weighted Entropy}
	
	
	
	\begin{frame}
	
	\begin{figure}
		\centering
		\includegraphics{../assets/decision-trees/figures/dt_weighted/fig3.pdf}
	\end{figure}
	
	
	Candidate Line: \(X1 = 4 (X1^*)\)
	
	
	\end{frame}
	
	
	\begin{frame}
	\begin{figure}
		\centering
		\includegraphics{../assets/decision-trees/figures/dt_weighted/fig4.pdf}
	\end{figure}
	
	Entropy of \(X1 \leq X1^*  = E_{S(X1 < X1^*)}\)
	
	$$P(+) = \frac{0.1 + 0.1}{0.1 + 0.1 + 0.3} = \frac{2}{5}$$
	
	$$P(-) = \frac{3}{5}$$
	
	
	\end{frame}
	
	
	\begin{frame}
	
	\begin{figure}
		\centering
		\includegraphics{../assets/decision-trees/figures/dt_weighted/fig5.pdf}
	\end{figure}
	Entropy of $X_1 > X_1^* = E_{S(X_1 > X_1^*)}$
	
	$$P(+) = \frac{3}{5}$$
	
	$$P(-) = \frac{2}{5}$$
	
	\end{frame}
	
	
	
	\begin{frame}
	
	\begin{figure}
		\centering
		\includegraphics{../assets/decision-trees/figures/dt_weighted/fig5.pdf}
	\end{figure}
	
	$$\text{IG}(X_1 = X_1^*) = E_S - \frac{0.5}{1} \cdot E_{S(X_1 < X_1^*)} - \frac{0.5}{1} \cdot E_{S(X_1 > X_1^*)}$$
	
	\end{frame}

\end{section}
\end{document}
