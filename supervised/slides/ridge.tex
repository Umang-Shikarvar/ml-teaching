\documentclass{beamer}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[labelformat=empty]{caption}
\usepackage{subcaption}
\usepackage{pdfpages}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 


%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}
	


\usetheme{metropolis}           % Use metropolis theme


\title{Ridge Regression}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle

\begin{frame}{Introduction}
A know measure of over-fitting can be the magnitude of the coefficient. \\ \bigskip
\pause
In $f(x) = c_0 + c_1x + c_2x^2 + \dots$ it is $\max|c_i|$ \\ \bigskip
\end{frame}
  
\begin{frame}{Introduction}
\vspace{0.4cm}
\only<1>{\begin{figure}\includegraphics[width=0.8\linewidth]{lin_1.pdf}\caption{Base Data Set}
\end{figure}}
\only<2>{\begin{figure}\includegraphics[width=0.8\linewidth]{lin_plot_1.pdf}\caption{Fit with Degree 1}
\end{figure}}
\only<3>{\begin{figure}\includegraphics[width=0.8\linewidth]{lin_plot_3.pdf}\caption{Fit with Degree 3}
\end{figure}}
\only<4>{\begin{figure}\includegraphics[width=0.8\linewidth]{lin_plot_6.pdf}\caption{Fit with Degree 6}
\end{figure}}
\only<5>{\begin{figure}\includegraphics[width=0.8\linewidth]{lin_plot_11.pdf}\caption{Fit with Degree 11}
\end{figure}}
\end{frame}  

\begin{frame}{Introduction}
\vspace{0.4cm}
In the examples we notice that as the degree increase (as the prediction starts to overfit the base data), the maximum coefficient also increases.
%\vspace{-0.3cm}
\begin{figure}\includegraphics[width=0.7\linewidth]{lin_plot_coef.pdf}\caption{Trend of the coefficients}\end{figure}

\end{frame}
 
\begin{frame}{Introduction}
\vspace{0.4cm}
To prevent over fitting we place penalties on large $\theta_i$
\pause
 \\ \bigskip
\begin{tcolorbox}{Objective}
\begin{align*}
\text{Minimize } & \left(y-X\theta\right)^T\left(y-X\theta\right) \\
\text{s.t. } & \theta ^T\theta \leq S
\end{align*}
\end{tcolorbox}
\pause
This is equivalent to \vspace{-0.4cm}
$$
\text{Minimize } \left(y-X\theta\right)^T\left(y-X\theta\right) + \delta ^2\theta ^T\theta
$$
\end{frame}  

\begin{frame}{Introduction}
\begin{figure}
        \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\linewidth]{ridge_base_contour.pdf}
                \caption{Contour Plot}
        \end{subfigure}%
        \begin{subfigure}[b]{0.5\textwidth}
                \includegraphics[width=\linewidth]{ridge_base_surface.pdf}
                \caption{Surface Plot}
        \end{subfigure}%
        \caption{Visualization of the Example}
\end{figure}
\end{frame}

\begin{frame}{KKT Conditions}
To implement this we use KKT Conditions
\pause
\begin{align*}
\text{Minimize } & \left(y-X\theta\right)^T\left(y-X\theta\right) \\
\text{s.t. } & \theta ^T\theta \leq S \\
L\left(\theta, \mu \right) =& \left(y-X\theta\right)^T\left(y-X\theta\right) + \mu\left(\theta^T\theta - S\right)
\end{align*}
where, $\mu \geq 0$ (and $\mu = \delta^2$)\bigskip

\pause
\begin{columns}
\begin{column}{0.4\textwidth}
If $\mu = 0$ \\
There is no regularization \\
No effect on constraint
\end{column}
\pause
\begin{column}{0.4\textwidth}
If $\mu\neq 0$ \\
$\implies \theta^T\theta - S = 0$ 
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Effect of $\mu$}
\vspace{0.4cm}
\only<1>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge_1.pdf}\caption{$\mu = 1$}
\end{figure}}
\only<2>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge_10.pdf}\caption{$\mu = 10$}
\end{figure}}
\only<3>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge_1000.pdf}\caption{$\mu = 1000$}
\end{figure}}
\end{frame}

%\begin{frame}{Effect of $\mu$ on higher degree fits}
%\vspace{0.4cm}
%\only<1>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_1_16.pdf}\caption{$\mu = 1$ when Degree $= 16$}
%\end{figure}}
%\only<2>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_100_16.pdf}\caption{$\mu = 100$  when Degree $= 16$}
%\end{figure}}
%\only<3>{\begin{figure}\includegraphics[width=0.8\linewidth]{ridge/ridge_100000_16.pdf}\caption{$\mu = 100000$  when Degree $= 16$}
%\end{figure}}
%\end{frame}

\begin{frame}{Effect of $\mu$ - Regularization of Parameters}
\vspace{0.4cm}
\begin{figure}\includegraphics[width=0.8\linewidth]{rid_reg-without-lim.pdf}\caption{Comparing the magnitudes of the coefficients with varying $\mu$\\(on the \emph{Real Estate Data Set})}
\end{figure}
\pause Are $\theta_{i}$ all zero for high $\mu$?
\end{frame}

\begin{frame}{Effect of $\mu$ - Regularization of Parameters}
\vspace{0.4cm}
\begin{figure}\includegraphics[width=0.8\linewidth]{rid_reg-with-lim.pdf}\caption{Comparing the magnitudes of the coefficients with varying $\mu$\\(on the \emph{Real Estate Data Set})}
\end{figure}
\end{frame}

\begin{frame}{Analytical Method}
\begin{tcolorbox}{Ridge Objective}
\vspace{-0.4cm}
$$
\min_\theta \left(y-X\theta\right)^T\left(y-X\theta\right)+ \mu\theta^T\theta
$$
\end{tcolorbox}
\begin{align*}
\frac{\partial L\left(\theta, \mu\right)}{\partial \theta} &= 0 \\ 
\frac{\partial}{\partial \theta}\left\lbrace y^Ty - 2y^TX\theta + \theta^TX^TX\theta \right\rbrace +  \frac{\partial}{\partial \theta} \mu\theta^T\theta &= 0 \\
\implies -X^Ty + \left(X^TX + \mu I\right)\theta &= 0 \\
\implies \theta^* &= \left(X^TX + \mu I\right)^{-1}X^Ty
\end{align*}
\end{frame}

\begin{frame}{Bias/Variance}
\begin{columns}
\begin{column}{0.4\textwidth}
\begin{figure}
\includegraphics[width=\linewidth]{ridge_new_0_17.pdf}
\end{figure}
Fit High Order Polynomial \\
$\implies$ high variance \\
$\implies \, \mu \rightarrow 0$
\end{column}
\begin{column}{0.4\textwidth}
\begin{figure}
\includegraphics[width=\linewidth]{ridge_new_1_17.pdf}
\end{figure}
Fit High Order Polynomial \\
$\implies$ low variance \\
$\implies \, \mu \rightarrow \infty$
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Example}
\vspace{0.4cm}
\textbf{Q.)} Solve Regularized ($\mu = 2$) and Unregularized.
\vspace{-0.6cm}
\begin{figure}
\includegraphics[width=\linewidth]{temp.pdf}
\end{figure}
\end{frame}

\begin{frame}{Example: Unregularized}
\[
\theta = (X^{T}X)^{-1}(X^{T}y)
\]
\pause
\begin{align*}
X^{T}X &= \begin{bmatrix}
4 &10\\10&30
\end{bmatrix} \\
(X^{T}X)^{-1} &= \frac{1}{20} \begin{bmatrix}
30 & -10\\
-10& 4
\end{bmatrix}\\
X^{T}y &= \begin{bmatrix}
6\\
14
\end{bmatrix}
\end{align*}
\end{frame}


\begin{frame}{Example: Unregularized}
\vspace{0.4cm}
\begin{align*}
\begin{split}
\theta &= (X^{T}X)^{-1}(X^{T}y)\\
\begin{bmatrix}
\theta_{0}\\
\theta_{1}
\end{bmatrix} &= 
\begin{bmatrix}
2\\
(-1/5)
\end{bmatrix} 
\end{split}
\end{align*}
\vspace{-0.8cm}
\begin{figure}
\includegraphics[width=0.8\linewidth]{q_unreg.pdf}
\end{figure}
\end{frame}

\begin{frame}{Example: Regularized}
\[
\theta = (X^{T}X+\mu I)^{-1}(X^{T}y)
\]
\pause
\begin{align*}
X^{T}X &= \begin{bmatrix}
4 &10\\10&30
\end{bmatrix} \\
X^{T}X+\mu I &= \begin{bmatrix}
6 &10\\10&32
\end{bmatrix} \\
(X^{T}X+\mu I)^{-1} &= \frac{1}{92} \begin{bmatrix}
32 & -10\\
-10& 6
\end{bmatrix}\\
X^{T}y &= \begin{bmatrix}
6\\
14
\end{bmatrix}
\end{align*}
\end{frame}

\begin{frame}{Example: Regularized}
\vspace{0.4cm}
\begin{align*}
\begin{split}
\theta &= (X^{T}X+\mu I)^{-1}(X^{T}y) \\
\begin{bmatrix}
\theta_{0}\\
\theta_{1}
\end{bmatrix} &= 
\begin{bmatrix}
0.56\\
0.26
\end{bmatrix} 
\end{split}
\end{align*}
\vspace{-0.8cm}
\begin{figure}
\includegraphics[width=0.8\linewidth]{q_reg.pdf}
\end{figure}
\end{frame}

\begin{frame}{Multi-collinearity}
$(X^{T}X)^{-1}$ is not computable when $|X^{T}X|$ = 0.

This was a drawback of using linear regression

\begin{equation*}
X = \begin{bmatrix}
1 & 1& 2\\
1 & 2& 4\\
1 & 3& 6\\
\end{bmatrix}
\end{equation*}

The matrix X is not full rank. 
\end{frame}

\begin{frame}{Multi-collinearity}
But with ridge regression, the matrix to be inverted is $X^{T}X + \mu I$ and not $X^{T}X$.

\begin{equation*}
X^TX + \mu I = \begin{bmatrix}
3+\mu & 6& 12\\
6 & 14+\mu & 28\\
12 & 28& 56+\mu \\
\end{bmatrix}
\end{equation*}

The matrix $X^TX$ would be full rank for $\mu>0$ . 

\pause Another interpretation of ``regularisation''
\end{frame}


\begin{frame}{Extension of the analytical model}
For ridge with no penalty on $\theta_0$
$$
\hat{\theta} = \left(X^TX+\mu I^*\right)^{-1}X^Ty
$$
where, $$I = \begin{bmatrix}
    \color{red}0 & 0 & 0 & \dots  & 0 \\
    0 & 1& 0 & \dots  & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \dots  & 1
\end{bmatrix}$$
\end{frame}


{
	\setbeamercolor{background canvas}{bg=}
	\includepdf[page=-]{ridge-intercept.pdf}
}


{
	\setbeamercolor{background canvas}{bg=}
	\includepdf[page=-]{ridge-cv.pdf}
}

\begin{frame}{Ridge Solution using Gradient Descent}
\begin{itemize}[<+->]
	\item \(\theta=\theta - \alpha \frac{\partial}{\partial \theta}(\left(y-X\theta\right)^{\top}\left(y-X\theta\right)+\mu\theta^{\top}\theta)\) 
	\item \(\theta=\theta - \alpha(-2 X^{\top} y+2 X^{\top} X \theta + 2\mu I \theta)\)
	\item \(\theta=(1-2\alpha\mu I)\theta - \alpha(-2 X^{\top} y+2 X^{\top} X \theta)\)
	\item \(\theta=\underbrace{(1-2\alpha\mu I)\theta}_\text{Shrinking $\theta$} - \alpha(-2 X^{\top} y+2 X^{\top} X \theta)\)
\end{itemize}
\pause \begin{itemize}

	\item Contrast with update equation for unregularised regression:
	\item \(\theta=\underbrace{\theta}_\text{No Shrinking $\theta$} - \alpha(-2 X^{\top} y+2 X^{\top} X \theta)\)
	
\end{itemize}

\end{frame}

{
	\setbeamercolor{background canvas}{bg=}
	\includepdf[page=-]{ridge-gd.pdf}
}

\end{document}