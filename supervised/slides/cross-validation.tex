\documentclass[usenames,dvipsnames]{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}

% Counter for pop quizzes

\title{Cross-Validation}
\date{\today}
\author{Nipun Batra and teaching staff}
\institute{IIT Gandhinagar}

\begin{document}
	\maketitle

\section{Introduction to Cross-Validation}

\begin{frame}Does not use the full dataset for training and does not test on the full dataset
	\item \pause No way to optimize hyperparameters
	\item \pause This simple train/test split has limitations we need to address
\end{itemize}
\end{frame}

\begin{frame}\begin{block}{Answer}
\begin{itemize}
\item Does not utilize the full dataset for training
	\item Cannot optimize hyperparameters systematically
	\pause
\item Results depend on the particular split chosen
	\item May not get reliable performance estimates
\end{itemize}
\end{block}
\end{frame}

\section{Full Dataset Utilization}

\begin{frame}Over multiple iterations, use different parts of the dataset for training and testing
	\item \pause Typically done via different random splits of the dataset
	\item \pause \textbf{Challenge:} How to ensure systematic evaluation?
	\item \pause May not use every data point for training or testing with random splits
	\item \pause May be computationally expensive
\end{itemize}
\end{frame}

\section{K-Fold Cross-Validation}

\begin{frame}\begin{itemize}
\item Each data point is used for testing exactly once
	\item Each data point is used for training $(k-1)/k$ of the time
	\pause
\item Provides more robust performance estimates
\end{itemize}
\end{frame}

\begin{frame}\begin{block}{Answer}
80 data points (4 out of 5 folds = 4/5 Ã— 100 = 80)
\end{block}
\end{frame}

\section{Hyperparameter Optimization}

\begin{frame}Validation set helps select the best hyperparameters
	\item \pause Test set remains untouched until final evaluation
	\item \pause This prevents overfitting to the test set
\end{itemize}
\end{frame}

\section{Nested Cross-Validation}

\begin{frame}Each fold provides one validation score
	\item \pause Process is systematic and exhaustive
\end{itemize}
\end{frame}

\begin{frame}\begin{block}{Answer}
\begin{itemize}
\item Simple CV: Used for model evaluation only
	\item Nested CV: Outer loop for model evaluation, inner loop for hyperparameter tuning
	\pause
\item Nested CV provides unbiased estimates when doing hyperparameter search
\end{itemize}
\end{block}
\end{frame}

\begin{frame}Final model is trained on entire training set
	\item \pause Standard deviation gives confidence in results
\end{itemize}
\end{frame}

\begin{frame}\begin{block}{Answer}
\begin{itemize}
\item Single fold results can be misleading due to data variance
	\item Averaging provides more robust performance estimates
	\pause
\item Reduces impact of lucky/unlucky splits
	\item Standard deviation indicates reliability of the estimate
\end{itemize}
\end{block}
\end{frame}

\section{Cross-Validation Variants}

\begin{frame}Special case where $k = n$ (number of data points)
	\item \pause Each fold uses exactly one data point for testing
	\item \pause \textbf{Advantages:}
	\begin{itemize}
\item Maximum use of data for training
		\item Deterministic (no randomness)
	\end{itemize}
	\item \pause \textbf{Disadvantages:}
	\begin{itemize}
\item Computationally expensive
		\item High variance in estimates
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}Maintains class distribution in each fold
	\item \pause Important for imbalanced datasets
	\item \pause Each fold has approximately same proportion of classes
	\item \pause \textbf{Example:} If dataset is 70\% class A, 30\% class B, each fold maintains this ratio
	\item \pause Reduces variance in performance estimates
\end{itemize}
\end{frame}

\begin{frame}\begin{block}{Answer}
\begin{itemize}
\item Regular CV might create folds with very few (or zero) positive examples
	\item This would give misleading performance estimates
	\pause
\item Stratified CV ensures each fold has $\sim$10\% positive examples
	\item Results in more reliable and consistent evaluation
\end{itemize}
\end{block}
\end{frame}

\section{Time Series Cross-Validation}

\begin{frame}Regular CV assumes data points are independent
	\item \pause Time series data has temporal dependencies
	\item \pause \textbf{Forward Chaining:} Train on past, test on future
	\item \pause \textbf{Rolling Window:} Fixed-size training window
	\item \pause \textbf{Expanding Window:} Growing training set over time
	\item \pause Never use future data to predict past!
\end{itemize}
\end{frame}

\section{Common Pitfalls and Best Practices}

\begin{frame}\textbf{Data Leakage:} Information from test set influences training
	\item \pause \textbf{Incorrect Splitting:} Not accounting for grouped data
	\item \pause \textbf{Overfitting to CV:} Too much hyperparameter tuning
	\item \pause \textbf{Wrong Preprocessing:} Scaling on entire dataset before splitting
	\item \pause \textbf{Ignoring Class Imbalance:} Not using stratified CV when needed
\end{itemize}
\end{frame}

\begin{frame}\begin{block}{Answer}
\begin{itemize}
\item This causes data leakage!
	\item Test fold statistics influence the training preprocessing
	\pause
\item Should compute statistics only on training folds
	\item Apply same transformation to corresponding test fold
	\item This gives more realistic performance estimates
\end{itemize}
\end{block}
\end{frame}

\section{Summary and Key Takeaways}

\begin{frame}\textbf{Better Data Utilization:} Every point used for both training and testing
	\item \pause \textbf{Robust Evaluation:} Multiple train/test splits reduce variance
	\item \pause \textbf{Hyperparameter Tuning:} Systematic way to select best parameters
	\item \pause \textbf{Model Comparison:} Fair comparison between different algorithms
	\item \pause \textbf{Confidence Estimates:} Standard deviation indicates reliability
\end{itemize}
\end{frame}

\begin{frame}\textbf{K-Fold (k=5,10):} General purpose, most common
	\item \pause \textbf{Stratified:} Imbalanced classification problems
	\item \pause \textbf{LOOCV:} Small datasets, when computational cost is acceptable
	\item \pause \textbf{Time Series CV:} Temporal data with dependencies
	\item \pause \textbf{Nested CV:} When doing extensive hyperparameter search
\end{itemize}
\end{frame}

\begin{frame}Always preprocess within each fold separately
	\item \pause Use stratification for classification problems
	\item \pause Report mean $\pm$ standard deviation
	\item \pause Don't overfit to cross-validation results
	\item \pause Consider computational cost vs. benefit trade-off
	\item \pause Use nested CV for unbiased hyperparameter search
\end{itemize}
\end{frame}

\begin{frame}{Next time: Ensemble Learning}
\begin{itemize}
\item How to combine various models?
\item Why combine multiple models?
\pause
\item How can we reduce bias?
\item How can we reduce variance?
\item Bootstrap aggregating (Bagging)
\pause
\item Boosting methods
\end{itemize}
\end{frame}

\end{document}
