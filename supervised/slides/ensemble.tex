\documentclass{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}

%\beamerdefaultoverlayspecification{<+->}
% \newcommand{\data}{\mathcal{D}}
% \newcommand\Item[1][]{%
%   \ifx\relax#1\relax  \item \else \item[#1] \fi
%   \abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}

\newcommand*{\Comb}[2]{{}^{#1}C_{#2}}%

\graphicspath{ {../assets/ensemble/figures/} }

\title{Ensemble Learning}
\date{\today}
\author{Nipun Batra and teaching staff}
\institute{IIT Gandhinagar}
\begin{document}
\maketitle

\begin{frame}Based on \hyperlink{https://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf}{Ensemble methods in ML by Dietterich}

  Three reasons why ensembles make sense:

  \pause 1) Statistical: Sometimes if \textbf{data is limited, many competing hypotheses can be learned} all giving the same accuracy on training data.

  \pause E.g., we can learn many decision trees for the same data giving the same accuracy.

  \includegraphics[scale=0.2]{../assets/ensemble/diagrams/statistical.jpg}

\end{frame}
  
\begin{frame}2) Computational: Even if data is sufficient, some \textbf{classifiers/regressors can get stuck in local optima or apply greedy strategies}. Computationally learning the ``best'' hypothesis can be non-trivial.

  \pause E.g., decision trees employ greedy criteria

  \includegraphics[scale=0.2]{../assets/ensemble/diagrams/computational.jpg}

\end{frame}

\begin{frame}3) Representational: Some \textbf{classifiers/regressors cannot learn the true form or representation.}

  \pause E.g., decision trees can only learn axis-parallel splits.

  \includegraphics[scale=0.2]{../assets/ensemble/diagrams/representational.jpg}

\end{frame}

\begin{frame}1) A necessary and sufficient condition for an ensemble of classifiers to be more
  accurate than any of its individual members is if the classifiers are accurate and
  diverse.

  \pause 2) An accurate classifier: \pause  is one that has an
  error rate of better than random guessing on new x values.

  \pause 3) Two classifiers are diverse: \pause  if they make different errors on new data points

\end{frame}

\begin{frame}If the three classifiers are identical, i.e.
  not diverse, then when $h_1(x)$ is wrong $h_2(x)$ and $h_3(x)$ will also be wrong.

  \pause However, if the errors made by the classifiers are uncorrelated, then when $h_1(x)$
  is wrong, $h_2(x)$ and $h_3(x)$ may be correct, so that a majority vote will correctly
  classify.

\end{frame}
\begin{frame}Error Probability of each model = $\varepsilon$ = 0.3\\
  \vspace{1cm}
  $Pr(\text{ensemble being wrong}) = \Comb{3}{2}(\varepsilon^2)(1-\varepsilon)^{3-2} + \Comb{3}{3}(\varepsilon^3)(1-\varepsilon)^{3-3}$\\
  \vspace{0.5cm}
  \hspace{4.4cm}$ = 0.19 \leq 0.3$
\end{frame}

\begin{frame}\begin{column}{0.3\textwidth}
      \centering
      Round - 1\\

      \includegraphics[width = 0.9\textwidth]{../assets/ensemble/figures/dataset-rnd-0}

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 2\\

      \includegraphics[width = 0.9\textwidth]{../assets/ensemble/figures/dataset-rnd-1}

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 3\\

      \includegraphics[width = 0.9\textwidth]{../assets/ensemble/figures/dataset-rnd-2}

    \end{column}

  \end{columns}
  \vspace{0.5cm}
  \begin{columns}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 4\\

      \includegraphics[width = 0.9\textwidth]{../assets/ensemble/figures/dataset-rnd-3}

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 5\\

      \includegraphics[width = 0.9\textwidth]{../assets/ensemble/figures/dataset-rnd-4}

    \end{column}

  \end{columns}
\end{frame}

\begin{frame}\begin{column}{0.3\textwidth}
      \centering
      Round - 1\\

      \includegraphics[width = 0.9\textwidth]{../assets/ensemble/figures/decision-boundary-0}
      Tree Depth = 4

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 2\\

      \includegraphics[width = 0.9\textwidth]{../assets/ensemble/figures/decision-boundary-1}
      Tree Depth = 5

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 3\\

      \includegraphics[width = 0.9\textwidth]{../assets/ensemble/figures/decision-boundary-2}
      Tree Depth = 5

    \end{column}

  \end{columns}
  \vspace{0.5cm}
  \pause  \begin{columns}
    \begin{column}{0.3\textwidth}
      \centering
      Round - 4\\

      \includegraphics[width = 0.9\textwidth]{../assets/ensemble/figures/decision-boundary-3}
      Tree Depth = 2

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 5\\

      \includegraphics[width = 0.9\textwidth]{../assets/ensemble/figures/decision-boundary-4}
      Tree Depth = 4

    \end{column}

  \end{columns}
\end{frame}

\begin{frame}\item All learners are incrementally built.
          \pause \item Incremental building: Incrementally try to classify ``harder'' samples correctly.
  \end{itemize}
\end{frame}

\begin{frame}\begin{column}{0.33\textwidth}
				\centering
				\begin{figure}
					\includegraphics[width = \textwidth]{../assets/ensemble/diagrams/ada_iter1_misclassify}
									\vspace{-20pt}
					\caption{$\alpha_1=0.42$}
				\end{figure}
				
			\end{column}

			\pause \begin{column}{0.4\textwidth}
				\centering
				\begin{figure}
					\includegraphics[width = \textwidth]{../assets/ensemble/diagrams/ada_iter2_misclassify}
					\vspace{-20pt}
					\caption{$\alpha_2=0.66$}
				\end{figure}
	
			\pause \end{column}
					\begin{column}{0.3\textwidth}
				\centering
				\begin{figure}
					\includegraphics[width = \textwidth]{../assets/ensemble/diagrams/ada_iter3_misclassify}
									\vspace{-20pt}
					\caption{$\alpha_3=0.99$}
				\end{figure}
				
			\end{column}
		\end{columns}
	
		\begin{columns}
			\pause \begin{column}{0.5\textwidth}
				\begin{figure}
				\includegraphics[scale=0.1]{../assets/ensemble/diagrams/testing}
				\end{figure}
			\end{column}
		
		\begin{column}{0.5\textwidth}
			\pause Let us say, yellow class is +1 and blue class is -1
			
			\pause Prediction = SIGN(0.42*-1 + 0.66*-1 + 0.99*+1) = Negative = blue
		\end{column}
		\end{columns}
	
\end{frame}

% \begin{frame}\begin{column}{0.4\textwidth}
%       \centering
%       \begin{figure}
%         \includegraphics[width = \textwidth]{../assets/ensemble/diagrams/ada_iter2_misclassify}
%         \vspace{-20pt}
%         \caption{$\alpha_2=0.66$}
%       \end{figure}
% 	\end{column}
% \end{columns}

% \end{frame}
		
% 	\begin{frame}\end{column}
%     \begin{column}{0.3\textwidth}
%       \centering
%       \begin{figure}
%         \includegraphics[width = \textwidth]{../assets/ensemble/diagrams/ada_iter3_misclassify}
%         \vspace{-20pt}
%         \caption{$\alpha_3=0.99$}
%       \end{figure}

%     \end{column}
%   \end{columns}

%   \begin{columns}
%     \pause \begin{column}{0.5\textwidth}
%       \begin{figure}
%         \includegraphics[scale=0.1]{../assets/ensemble/diagrams/testing}
%       \end{figure}
%     \end{column}

%     \begin{column}{0.5\textwidth}
%       \pause Let us say, yellow class is +1 and blue class is -1

%       \pause Prediction = SIGN(0.42*-1 + 0.66*-1 + 0.99*+1) = Negative = blue
%     \end{column}
%   \end{columns}

% \end{frame}

\begin{frame}\begin{column}{0.5\textwidth}

      \begin{figure}[htp]
        \centering
        \begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/boosting-explanation.html}
          % \includegraphics[scale=0.8]{../figures/decision-trees/entropy.pdf}
          \includegraphics[scale=0.55]{../assets/ensemble/figures/alpha-boosting.pdf}
        \end{notebookbox}
      \end{figure}
    \end{column}
    \pause \begin{column}{0.5\textwidth}
      \begin{figure}[htp]
        \centering
        \begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/boosting-explanation.html}
          \includegraphics[scale=0.55]{../assets/ensemble/figures/alpha-boosting-weight.pdf}
        \end{notebookbox}
      \end{figure}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}{ADABoost for regresion}
  From Paper: Improving Regressors using Boosting Techniques

  \includegraphics[scale=0.1]{../assets/ensemble/diagrams/adaboost-regression.jpg}
  
\end{frame}

\begin{frame}{Random Forest}
  \begin{itemize}
\item Random Forest is an ensemble of decision trees.
    \item We have two types of bagging: bootstrap (on data) and random subspace (of features).
    \pause
\item As features are randomly selected, we learn decorrelated trees and helps in reducing variance.
  \end{itemize}
\end{frame}

\begin{frame}{Random Forest}
There are 3 parameters while training a random forest number of trees, number of features (m), maximum depth.\\
\vspace{1cm}
\underline{Training Algorithm}\\
\begin{itemize}
\item For $i^{th}$ tree ($i \in \{1 \cdots N\}$), select $n$ samples from total $N$ samples with replacement.\\
%   \item for $tree$ in $1, \dots,$ $number$ $of$ $trees$ $\]$
		
		\pause
\item Learn Decision Tree on selected samples for $i^{th}$ round.

\end{itemize}

\underline{Learning Decision Tree (for RF)}\\
\begin{itemize}
\item For each split, select $m$ features from total available $M$ features and train a decision tree on selected features
\end{itemize}

\end{frame}

\begin{frame}{Dataset }
  \includegraphics[scale=0.5]{../assets/ensemble/diagrams/dataset-iris.png}
\end{frame}

% \begin{frame}{Random Forest}
% There are 3 parameters while training a random forest ``$number$ $of$ $trees$'', ``$number$ $of$ $features'' (m)$, ``$maximum$ $depth$''.\\
% \vspace{1cm}
% \underline{Training Algorithm}\\
% \begin{itemize}
\item for $depth$ in $[1, \dots,$ $maximum$ $depth$ $]$
%     \begin{itemize}
%       \item for $tree$ in $[1, \dots,$ $number$ $of$ $trees$ $]$
%       \begin{itemize}
%         \pause
\item For each split, select ``$m$'' features from total available $M$ features and train a decision tree on selected features
%         
%       \end{itemize}
%     \end{itemize}
% \end{itemize}
% \end{frame}

\newcounter{tree}
\forloop{tree}{0}{\value{tree} < 10}{
  \begin{frame}{Decision Tree \# \thetree}
    \begin{figure}
      % \includegraphics[scale=0.25]{../assets/ensemble/figures/feature-imp-\thetree.pdf}
      \centering
      \begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/ensemble-feature-importance.html}
        \includegraphics[scale=0.25]{../assets/ensemble/figures/feature-imp-\thetree.pdf}
        
      \end{notebookbox}
    \end{figure}
  \end{frame}
}

\begin{frame}{Feature Importance\footnotemark}
  \begin{figure}
    \includegraphics[scale=0.4]{../assets/ensemble/diagrams/mdi.pdf}
  \end{figure}
  Importance of variable $X_j$ for an ensemble of $M$ trees $\varphi_{m}$ is:
  \begin{equation*}
    \text{Imp}(X_j) = \frac{1}{M} \sum_{m=1}^M \sum_{t \in \varphi_{m}} 1(j_t = j) \Big[ p(t) \Delta i(t) \Big],
  \end{equation*}
  where $j_t$ denotes the variable used at node $t$, $p(t)=N_t/N$ and $\Delta i(t)$ is the impurity reduction at node $t$:
  \begin{equation*}
    \Delta i(t) = i(t) - \frac{N_{t_L}}{N_t} i(t_L) - \frac{N_{t_r}}{N_t} i(t_R)
  \end{equation*}
  \footnotetext[1]{Slide Courtesy Gilles Louppe}

\end{frame}

\begin{frame}{Computed Feature Importance}
  \begin{figure}[htp]
    \centering
    \begin{notebookbox}{https://nipunbatra.github.io/ml-teaching/notebooks/ensemble-feature-importance.html}
      \includegraphics[scale=0.8]{../assets/ensemble/figures/feature-imp-forest.pdf}
    \end{notebookbox}
\end{figure}
  % \includegraphics[scale=0.6]{feature-importance.pdf}
\end{frame}

\end{document}
