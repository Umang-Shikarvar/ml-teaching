\documentclass[usenames,dvipsnames]{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}

\usepackage{color, colortbl}
\usepackage{comment}

%\beamerdefaultoverlayspecification{<+->}

\title{K-Nearest Neighbors}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
\maketitle

\begin{frame}\textbf{Low values of K} will result in each point having a very high influence on the final output $\implies$ noise will influence the result
\pause

\textbf{High values of K} will result in smoother decision boundaries \\$\implies$ lower variance but also higher bias
\end{frame}

\begin{frame}\item Why is feature scaling important for KNN?
\pause
\item In which scenarios would you prefer KNN over parametric methods?
\pause
\item What is the time complexity of finding $k$ nearest neighbors naively?
\end{enumerate}
\end{frame}

\begin{frame}{Key Takeaways}
\begin{itemize}
\item \textbf{Non-parametric}: KNN makes no assumptions about data distribution
\pause
\item \textbf{Lazy Learning}: No training phase, computation happens at prediction time
\pause
\item \textbf{Choice of $k$}: Small $k$ → high variance, large $k$ → high bias
\pause
\item \textbf{Distance Metrics}: Choice affects performance significantly
\pause
\item \textbf{Curse of Dimensionality}: Performance degrades in high dimensions
\pause
\item \textbf{Scalability}: Approximate methods needed for large datasets
\end{itemize}
\end{frame}

\end{document}
