\documentclass{beamer}
\usepackage{tcolorbox}

%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}
\usepackage{amsmath}

% limits underneath
\DeclareMathOperator*{\argminA}{arg\,min} % Jan Hlavacek
\DeclareMathOperator*{\argminB}{argmin}   % Jan Hlavacek
\DeclareMathOperator*{\argminC}{\arg\min}   % rbp

\newcommand{\argminD}{\arg\!\min} % AlfC

\newcommand{\argminE}{\mathop{\mathrm{argmin}}}          % ASdeL
\newcommand{\argminF}{\mathop{\mathrm{argmin}}\limits}   % ASdeL

% limits on side
\DeclareMathOperator{\argminG}{arg\,min} % Jan Hlavacek
\DeclareMathOperator{\argminH}{argmin}   % Jan Hlavacek
\newcommand{\argminI}{\mathop{\mathrm{argmin}}\nolimits} % ASdeL

\usetheme{metropolis}           % Use metropolis theme


\title{Some Practical Tips And Feature Selection}
\date{\today}
\author{Nipun Batra and teaching staff}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle
  
  
  
% \section{Linear Regression}


\begin{frame}{Ideas for Baselines}
    
    
    \begin{itemize}[<+->]
        \item Mean Model: $\hat{y}$= Mean of the training set
        \item Median or Mode of the training set
        \item Random (Min(training set), Max(training set))
    \end{itemize}
\end{frame}


\begin{frame}{Choosing Best features}
    To find the best set of features, one can do exhaustive enumeration (brute force) of features.    
    
    \pause \begin{tabular}{c|c|c|c}
        $Feature_{1}$ & $Feature_{2}$ &\dots & $Feature_{d}$  \\
        \hline
      
         True  & False &  \dots & False\\
         False &  True & \dots & False\\ 
         True &  True & \dots & False\\ 
         \vdots &  \vdots & \vdots&\\
         True & True &\dots & True\\ 
         \hline 
         
    \end{tabular}
    
    \pause The entries of the table denote if the feature is used for creating a model. In total we have $2^{d}$ models: training models using exhaustive enumeration is very expensive!
\end{frame}


\begin{frame}{Stepwise Forward Selection}

    $F$ = $\{\}$\\
    for i = 1 to K\\
    \vspace{1em}
    \hspace{2em} $F_{i}$  = $\argminF_{feature \notin F} $ Loss(F $\cup$ feature)\\
    \vspace{1em}
    \hspace{2em} $F$ = $F \cup F_{i}$ \\
        
\vspace{2em}
Loss($features$) denotes the loss incurred by the model trained with $features$.
\end{frame}



\begin{frame}{Stepwise Forward Selection for California Housing Data }

	Now we will be doing SFS on the California Housing Dataset. We will try to predict the median-selling price(in thousands of dollars) for households in the neighbourhood.
	
\end{frame}

\begin{frame}{Stepwise Forward Selection for California Housing Data }
\begin{center}

	\begin{tabular}{|c|c|c|}
	\hline
	\textbf{Iteration}&	\textbf{Added Feature}	&\textbf{MSE}\\
	\hline
	\hline
	1&Median Income of block&0.97\\
	2&Avg. number  of rooms in the block&0.63\\
	3&Latitude&0.65\\
	4&Longitude&0.66\\
	\hline
	
	
	\end{tabular}


\end{center}

	This shows except the first two features, everything else are unimportant features.
\end{frame}

\begin{frame}{Stepwise Backward Selection}
    Same as SFS, but in opposite direction\\
    Remove feature, which reduces the accuracy the least(uninmportant).
\end{frame}

\begin{frame}{Time Complexity Analysis}
    
    Both SFS and SBS are $O(d^2)$ algorithms, where d is the number of features.
    
     \begin{equation*}
         \implies (d) + (d-1) + (d-2) + \dots + (1)
           \end{equation*}
     
     \begin{equation*}
          \implies \cfrac{d(d-1)}{2}
            \end{equation*}
     
     \begin{equation*}
         \implies d(d-1)  \implies d^{2}
           \end{equation*}
\end{frame}
\end{document}