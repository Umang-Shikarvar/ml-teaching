\documentclass{beamer}
\usepackage{../../shared/styles/custom}
\usepackage{../../shared/styles/conventions}

%\beamerdefaultoverlayspecification{<+->}
% \newcommand{\data}{\mathcal{D}}
% \newcommand\Item[1][]{%
% 	\ifx\relax#1\relax  \item \else \item[#1] \fi
% 	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}

\title{Logistic Regression}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
\maketitle

\section{Problem Setup}

\begin{frame}{Classification Technique}
%\vspace{5cm}
\begin{minipage}{0.3\textwidth}
	% Show the image at item three and afterwards
	
	\begin{figure}
		
		% From https://i.imgur.com/AyzVOIO.jpg
		\includegraphics{../assets/logistic-regression/figures/logistic-orange-tomatoes-original.pdf}
	\end{figure}
\end{minipage} \\

\end{frame}

\begin{frame}{Classification Technique}
%\vspace{5cm}
\begin{minipage}{0.3\textwidth}
	% Show the image at item three and afterwards
	
	\begin{figure}
		
		% From https://i.imgur.com/AyzVOIO.jpg
		\includegraphics{../assets/logistic-regression/figures/logistic-orange-tomatoes.pdf}
	\end{figure}
\end{minipage} \\
%\vspace{-1.7cm}
\pause Aim: Probability(Tomatoes $|$ Radius) ? or
\pause \framebox{More generally, P($y = 1 | X = x$)?}

\end{frame}

\begin{frame}{Idea: Use Linear Regression}
\hspace{2.5cm}
\begin{minipage}{0.3\textwidth}
	% Show the image at item three and afterwards
	
	\begin{figure}
		
		% From https://i.imgur.com/AyzVOIO.jpg
		\includegraphics{../assets/logistic-regression/figures/linear-orange-tomatoes.pdf}
	\end{figure}
\end{minipage} \\
\begin{equation*}
P(X = Orange | Radius) = \theta_{0} + \theta_{1} \times Radius
\end{equation*}
\pause Generally,
\begin{equation*}
P(y = 1 | x) = X\theta
\end{equation*}
\end{frame}
\begin{frame}{Idea: Use Linear Regression}
Prediction:\\
If $\theta_{0} + \theta_{1}\times Radius > 0.5 \rightarrow$ Orange \\
\hspace{3.3cm} Else $\rightarrow$ Tomato\\
Problem:\\
Range of $X\theta$ is $(-\infty, \infty)$\\
But $P(y = 1 | \ldots) \in [0, 1]$
\end{frame}

\begin{frame}{Idea: Use Linear Regression}
\includegraphics{../assets/logistic-regression/figures/linear-orange-tomatoes-decision.pdf}
\end{frame}

\begin{frame}{Idea: Use Linear Regression}
\includegraphics{../assets/logistic-regression/figures/linear-orange-tomatoes-decision-modified.pdf}


Linear regression for classification gives a poor prediction!
\end{frame}

\begin{frame}{Ideal boundary}
\includegraphics{../assets/logistic-regression/figures/linear-orange-tomatoes-decision-ideal.pdf}

\begin{itemize}[<+->]
	\item Have a decision function similar to the above (but not so sharp and discontinuous)
	\item Aim: use linear regression still!
\end{itemize}
\end{frame}

\begin{frame}{Idea: Use Linear Regression}
\hspace{1cm}
\begin{minipage}{0.3\textwidth}
% Show the image at item three and afterwards
\includegraphics{../assets/logistic-regression/figures/logistic.pdf}
\end{minipage} \\
Question. Can we still use Linear Regression? \\
Answer. Yes! Transform $\hat{y} \rightarrow [0, 1]$
\end{frame}

\section{Logistic/Sigmoid function}

\begin{frame}{Logistic / Sigmoid Function}
$\hat{y} \in (-\infty, \infty)$ \\
$\phi =$ Sigmoid / Logistic Function $(\sigma)$ \\
$\phi(\hat{y}) \in [0, 1]$
\begin{equation*}
\sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation*}
\includegraphics{../assets/logistic-regression/figures/logistic-function.pdf}
\end{frame}

\begin{frame}{Logistic / Sigmoid Function}
 $z \rightarrow \infty$\\
\pause  $\sigma(z) \rightarrow 1$\\
\pause $z \rightarrow -\infty$\\
 \pause $\sigma(z) \rightarrow 0$\\
 \pause $z = 0$\\
 \pause $\sigma(z) = 0.5$

\end{frame}

\begin{frame}\begin{equation*}
P(y = 0 | X) = 1 - P(y = 1 | X) = 1 - \frac{1}{1 + e^{-\mX\vtheta}} = \frac{e^{-\mX\vtheta}}{1 + e^{-\mX\vtheta}} 
\end{equation*}

\pause \begin{equation*}
\therefore \frac{P(y = 1|X)}{1 - P(y = 1|X)} = e^{\mX\vtheta}
\implies \mX\vtheta = \log\frac{P(y = 1|X)}{1 - P(y = 1 | X)}
\end{equation*}
\end{frame}
\begin{frame}\textbf{Why?} Squared loss + sigmoid creates non-convex surface:
\begin{itemize}
\item Sigmoid $\sigma(z) = \frac{1}{1+e^{-z}}$ is non-linear
\item Composition $(\sigma(\mX\vtheta) - y)^2$ has multiple local minima
\pause
\item No guarantee gradient descent finds global optimum
\item \textcolor{red}{This is why we need cross-entropy loss instead!}
\end{itemize}
\end{frame}

\section{Deriving Cost Function via Maximum Likelihood Estimation}

\begin{frame}This cost function is called cross-entropy.

\pause Why?
\end{frame}

\begin{frame}What is the interpretation of the cost function?

\pause Let us try to write the cost function for a single example:

\pause $$J(\theta) = -y_i\log{\hat{y}_i} - (1-y_i)\log({1-\hat{y}_i})$$

\pause First, assume $y_i$ is 0, then if $\hat{y}_i$ is 0, the loss is 0; but, if $\hat{y}_i$ is 1, the loss tends towards infinity!

	\includegraphics[scale=0.7]{../assets/logistic-regression/figures/logistic-cross-cost-0}

\end{frame}

\begin{frame}What is the interpretation of the cost function?

 $$J(\theta) = -y_i\log{\hat{y}_i} - (1-y_i)\log({1-\hat{y}_i})$$

\pause Now, assume $y_i$ is 1, then if $\hat{y}_i$ is 0, the loss is huge; but, if $\hat{y}_i$ is 1, the loss is zero!

\includegraphics[scale=0.7]{../assets/logistic-regression/figures/logistic-cross-cost-1}

\end{frame}

\begin{frame}Bias! 
\end{frame}

\begin{frame}\includegraphics{../assets/logistic-regression/figures/logisitic-circular-probability.pdf}

\end{frame}

\begin{frame}How would you learn a classifier? Or, how would you expect the classifier to learn decision boundaries?
\end{frame}

\begin{frame}\begin{enumerate}
\item Use one-vs.-all on \underline{Binary} Logistic Regression
\item Use one-vs.-one on \underline{Binary} Logistic Regression
\item Extend \underline{Binary} Logistic Regression to \underline{Multi-Class} Logistic Regression
\end{enumerate}
\end{frame}

\begin{frame}\begin{enumerate}
	\item Learn P(setosa (class 1)) = $\mathcal{F}({\mX\vtheta_1})$
	\item P(versicolor (class 2)) = $\mathcal{F}({\mX\vtheta_2})$
	\item P(virginica (class 3)) = $\mathcal{F}({\mX\vtheta_3})$
	\item Goal: Learn $\theta_i \forall i \in \{1, 2, 3\}$
	\item Question: What could be an $\mathcal{F}?$

\end{enumerate}

\end{frame}

\begin{frame}\begin{enumerate}
	\item Question: What could be an $\mathcal{F}?$
	\item Property: $\sum_{i=1}^{3}\mathcal{F}{({\mX\vtheta_i})} = 1$
	\item Also $\mathcal{F}(z)\in [0, 1]$
	\item Also, $\mathcal{F}(z)$ has squashing proprties: $R \mapsto [0, 1]$
\end{enumerate}

\end{frame}
\begin{frame}Let us calculate $-\sum_{k=1}^{3}y_i^k \log{\hat{y}_i^k} $

\pause  = $-(0\times \log(0.1) + 1\times \log(0.8) + 0\times \log(0.1))$

\pause Tends to zero

\end{frame}

\begin{frame}Let us calculate $-\sum_{k=1}^{3}y_i^k \log{\hat{y}_i^k} $

\pause  = $-(0\times \log(0.1) + 1\times \log(0.4) + 0\times \log(0.1))$

\pause High number! Huge penalty for misclassification!

\end{frame}

\begin{frame}More generally, 
\pause \begin{equation*}
J(\theta) = -\bigg\{\sum_{i=1}^{N}y_{i}\log(\hat{y}_i) + (1 - y_{i})\log(1 - \hat{y}_i)\bigg\}
\end{equation*}

\pause \begin{equation*}
J(\theta) = -\bigg\{\sum_{i=1}^{N}y_{i}\log(\hat{y}_i) + (1 - y_{i})\log(1 - \hat{y}_i)\bigg\}
\end{equation*}

Extend to K-class:
\begin{equation*}
J(\theta) = -\bigg\{\sum_{i=1}^{N}\sum_{k=1}^{K}y_{i}^k\log(\hat{y}_{i}^k)\bigg\}
\end{equation*}
\end{frame}
\begin{frame}\item What is the key difference between sigmoid and softmax functions?
\pause
\item Why do we use cross-entropy loss instead of squared error?
\pause
\item How does regularization help in logistic regression?
\end{enumerate}
\end{frame}

\begin{frame}{Key Takeaways}
\begin{itemize}
\item \textbf{Probabilistic Model}: Outputs probabilities via sigmoid function
\pause
\item \textbf{Linear Decision Boundary}: Creates linear separation in feature space
\pause
\item \textbf{Maximum Likelihood}: Optimized using gradient-based methods
\pause
\item \textbf{Cross-Entropy Loss}: Appropriate for classification problems
\pause
\item \textbf{No Closed Form}: Requires iterative optimization (gradient descent)
\pause
\item \textbf{Regularization}: L1/L2 help prevent overfitting
\end{itemize}
\end{frame}

\end{document}
